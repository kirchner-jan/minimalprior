<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jan Kirchner" />
  <meta name="dcterms.date" content="2021-11-08" />
  <title>minimalprior</title>
  <link rel="stylesheet" href="../../reset.css" />
  <link rel="stylesheet" href="../../index.css" />
</head>

<body>
  <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title"><a href="https://kirchner-jan.github.io/minimalprior/"
            style="text-decoration: none; color: inherit;">minimalprior</a></h1>
        <span class="subtitle">a spinoff</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">2021-11-08</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://universalprior.substack.com/">Jan
          Kirchner</a></td>
    </tr>
  </table>
  <nav id="TOC" role="doc-toc">
    <ul class="incremental">
      <li><a href="#neural-correlates" id="toc-neural-correlates"><strong>Neural
            correlates</strong></a></li>
      <li><a href="#behavior" id="toc-behavior"><strong>Behavior</strong></a></li>
      <li><a href="#high-level-considerations" id="toc-high-level-considerations"><strong>High-level
            considerations</strong></a></li>
    </ul>
  </nav>
  <p>Epistemic status: High uncertainty; This is exploratory work; Our
    goal is to provide possible research directions rather than offering
    solutions.</p>
  <p>This is shared work. Most of the part on neural correlates is by
    <a href="https://universalprior.substack.com/">Jan</a>. Most of the
    parts on behavior and high-level considerations are by <a href="https://www.mariushobbhahn.com/aboutme/">Marius</a>.
    You can
    also find the post <a
      href="https://www.lesswrong.com/posts/Bpw2HXjMa3GaouDnC/what-are-red-flags-for-neural-network-suffering">here</a>
    on the LessWrong forum.
  </p>
  <p><strong>Background</strong></p>
  <p>I (Marius) was listening to the <a
      href="https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/">80k
      podcast with Chris Olah</a> on his work on interpretability. When he
    talked about neural circuits, he said they found many circuits
    similar to structures in the human brain (some are also quite
    different). The question that I can’t unthink since then is “What
    evidence would we need to see in Neural Networks that would convince
    us of suffering in a morally relevant way?”. So I teamed up with Jan
    to explore the question.</p>
  <p>To clarify, we don’t care about whether they suffer in a similar
    way that humans do or to the same extent. The question that
    interests us is <em>“Do neural networks suffer?”</em> and, more
    importantly, <em>“How would we tell?”</em>.</p>
  <p>Broadly, we think there are three different approaches to this
    problem: a) Neural correlates, b) Behavioural data, and c)
    high-level considerations.</p>
  <p>In this post, we want to find out if there are any avenues that
    give us reasonable answers or if you just run into a wall, like with
    most other questions about suffering, patienthood, etc.</p>
  <p>It’s not super clear what we are looking for exactly. In general,
    we are looking for all kinds of evidence that can potentially update
    our probability of NN suffering.</p>
  <p><strong>Summary of the results:</strong></p>
  <ol class="incremental" type="1">
    <li><strong>Neural Correlates:</strong> Neuroscience helps to
      de-confuse some terminology, but has not (yet) unambiguously
      identified a unique neural correlate for suffering. Pain is much
      better characterized as a “homeostatic behavioral drive” with
      associated cognitive routines. Since pain usually leads to
      suffering, finding the homeostatic behavioral drive might also be
      indicative of suffering in NNs. Finally, the <a
        href="https://www.brandeis.edu/roybal/docs/PANAS-GEN_website_PDF.pdf">PANAS
        measure</a> from psychology might provide comparability to
      humans.</li>
    <li><strong>Behavior:</strong> Looking at the response of NNs to
      different stimuli might yield some evidence about their ability to
      suffer. In language models, it is unclear how to test for the
      existence of suffering. But testing if GPT-3, for example, has a
      consistent concept of suffering or observing its answers to the <a
        href="https://www.brandeis.edu/roybal/docs/PANAS-GEN_website_PDF.pdf">PANAS</a>
      questions might be a start. For RL agents we can create tests
      comparable to animal studies. For example, one could look at the
      reaction to negative stimuli, lesion studies, theory of mind, or
      self-awareness as potential evidence for the ability to suffer.</li>
    <li><strong>High-level considerations:</strong> We think that this
      is the most promising avenue. Firstly, the ability to suffer
      probably develops because it gives an evolutionary edge. Thus, it is
      plausible that artificial agents such as reinforcement learners
      would develop it given the right capabilities and conditions.
      Secondly, current NN architectures are much smaller than human
      brains or other biological neural networks if you compare the number
      of parameters with the number of neurons or synapses. Especially the
      architectures that would plausibly develop the ability to suffer,
      e.g., reinforcement learners, are still much smaller than tiny
      animals.</li>
  </ol>
  <p>Combining all of the above, we think it is more plausible that
    current NN architectures don’t suffer than that they do. However, we
    are pretty uncertain since we mix the fuzzy concept of suffering
    with NNs, which are also not well understood.</p>
  <h3 id="neural-correlates"><strong>Neural correlates</strong></h3>
  <p>First, <a href="https://www.jstor.org/stable/187355">to avoid
      specific philosophical tripwires</a><sup>[1]</sup>, we <a
      href="https://www.lesswrong.com/posts/9iA87EfNKnREgdTJN/conceptual-engineering-the-revolution-in-philosophy-you-ve">engineer
      our concepts</a> using insights from neuroscience. The hope is that
    we might be able to derive necessary or sufficient conditions (or at
    least bundles of features that strongly correlate with suffering)
    that generalize outside the realm of biology. Thus, a natural
    approach to investigating how suffering comes about is to look at
    how it comes about in the (human) brain. The neuroscience of
    pain/suffering is a mature scientific field with <a
      href="https://sci-hub.se/https://www.annualreviews.org/doi/full/10.1146/annurev.neuro.26.041002.131022">seminal
      work going back to the 1800s</a>, <a
      href="https://www.dovepress.com/journal-of-pain-research-journal">multiple</a>
    <a href="https://www.jpain.org/">dedicated</a> <a
      href="https://www.journals.elsevier.com/neurobiology-of-pain">journals</a><sup>[2]</sup>,
    and very few straightforward answers:
  </p>
  <div class="sidenote">
    <p>[2] </p>
    <p>“Journal of Pain” is a dope band name.</p>
  </div>
  <div class="sidenote">
    <p>[1] </p>
    <p>I was still taught “pain is c-fiber firing” as an actual,
      defensible position in my philosophy of mind course.</p>
  </div>
  <blockquote>
    <p><em>Pain is an enigma. It differs from the classical senses
        (vision, hearing, touch, taste, and smell) because it is both a
        discriminative sensation and a graded motivation (or behavioral
        drive). […] It can attain intolerable intensity, but it can
        disappear in the heat of battle. It is a universal human experience
        that is commonly generalized to psychic suffering of any sort. -<a
          href="https://sci-hub.se/https://www.annualreviews.org/doi/full/10.1146/annurev.neuro.26.041002.131022">Craig</a></em>
    </p>
  </blockquote>
  <p>While far from being settled science, insights from neuroscience
    provide a backdrop on which we can investigate pain/suffering more
    generally. We find it helpful to distinguish the following
    concepts:</p>
  <p><strong>Nociception</strong><sup>[3]</sup> <strong>.</strong></p>
  <div class="sidenote">
    <p>[3] </p>
    <p>Note that this definition makes no reference to mental states or
      cognitive processes.</p>
  </div>
  <blockquote>
    <p><em>Nociception is the neural process of encoding and processing
        noxious (​​i.e. harmful, poisonous, or very unpleasant) stimuli.
        Nociception refers to a signal arriving at the central nervous
        system as a result of the stimulation of specialised sensory
        receptors in the peripheral nervous system called nociceptors. -<a
          href="https://www.physio-pedia.com/Nociception">Physiopedia</a></em></p>
  </blockquote>
  <p>While we know a lot about nociception<sup>[4]</sup>, it is not
    the most useful concept for understanding pain. In particular, there
    are numerous examples of pain without nociception (<a href="https://en.wikipedia.org/wiki/Phantom_limb">phantom limb
      pain</a> and <a href="https://my.clevelandclinic.org/health/diseases/12056-pain-psychogenic-pain">psychogenic
      pain</a>) and nociception without pain (<a
      href="https://www.scientificamerican.com/article/why-is-it-that-eating-spi/#:~:text=The%20answer%20hinges%20on%20the%20fact%20that%20spicy%20foods%20excite%20the%20receptors%20in%20the%20skin%20that%20normally%20respond%20to%20heat.%20Those%20receptors%20are%20pain%20fibers%2C%20technically%20known%20as%20polymodal%20nociceptors.">spicy
      food</a>, <a href="https://en.wikipedia.org/wiki/Congenital_insensitivity_to_pain">Congenital
      insensitivity to pain</a>).</p>
  <div class="sidenote">
    <p>[4] </p>
    <p>The pathways that transport nociceptive stimuli into the cortex
      have been mapped out carefully: The initial pain stimulus is turned
      into neural activity via <a href="https://en.wikipedia.org/wiki/Nociceptor">mechanical
        receptors</a> and relayed (via <a href="https://en.wikipedia.org/wiki/Group_C_nerve_fiber">nociceptive
        fibers</a>) into the <a
        href="https://en.wikipedia.org/wiki/Pain#:~:text=The%20neospinothalamic%20tract%20carries,nuclei%20of%20the%20thalamus">thalamus</a>.
      From there, the signal spreads into a multitude of different brain
      areas (prominently the <a href="https://en.wikipedia.org/wiki/Anterior_cingulate_cortex">ACC</a>
      and the <a href="https://en.wikipedia.org/wiki/Insular_cortex">insular
        cortex</a>), but at this point, the process would not be called
      nociception anymore.</p>
  </div>
  <p><strong>Pain.</strong></p>
  <p>Pain is distinct from nociception. One <a
      href="https://www.cell.com/trends/neurosciences/fulltext/S0166-2236(03)00123-1?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0166223603001231%3Fshowall%3Dtrue">prominent
      theory</a> on pain is that<sup>[5]</sup>:</p>
  <div class="sidenote">
    <p>[5] </p>
    <p>Emphasis in the excerpt is mine.</p>
  </div>
  <blockquote>
    <p><em>the human feeling of pain is both a distinct sensation and a
        motivation – that is, a specific emotion that reflects homeostatic
        behavioral drive, similar to temperature, itch, hunger and
        thirst.</em></p>
  </blockquote>
  <p>The term <em>homeostatic</em> does a lot of heavy lifting here.
    Implicitly assumed is that there are certain “reference levels” that
    the body is <a href="https://slatestarcodex.com/2017/03/06/book-review-behavior-the-control-of-perception/">trying
      to maintain</a>, and pain is a set of behavioral and cognitive
    routines that execute when your body moves too far away from the
    reference level<sup>[6]</sup>. While this definition conveniently
    maps onto concepts from <a href="https://arxiv.org/abs/2006.05604">machine learning</a>, pain
    is (unfortunately) not sufficient for suffering (see f.e. <a
      href="https://en.wikipedia.org/wiki/Sadomasochism">sadomasochism</a>),
    and there is substantial debate on whether pain is necessary for
    suffering (see <a href="https://jme.bmj.com/content/47/3/175">here</a>). Thus, a
    neural network could exhibit all the neurological signs of pain but
    still <a href="https://en.wikipedia.org/wiki/Mad_pain_and_Martian_pain">experience
      it as pleasurable</a><sup>[7]</sup>. We need an additional component
    beyond pain, which we might call…</p>
  <div class="sidenote">
    <p>[7] </p>
    <p>This appears to be the core of the “mad pain and martian pain”
      argument by David Lewis.</p>
  </div>
  <div class="sidenote">
    <p>[6] </p>
    <p>An example would be “accidentally holding your hand in a flame”.
      The reference point for your body is “not on fire” and the strong
      perturbation “hand on fire” triggers a set of behavioral (pull hand
      from fire) and cognitive (direct attention to hand, evaluate danger
      level, consider asking for help or warning peers) routines.</p>
  </div>
  <p><strong>Suffering</strong></p>
  <blockquote>
    <p><em>Perhaps the foremost theoretical “blind spot” of current
        philosophy of mind is conscious suffering. Thousands of pages have
        been written about colour “qualia” and zombies, but almost no
        theoretical work is devoted to ubiquitous phenomenal states like
        boredom, the subclinical depression folk-psychologically known as
        “everyday sadness“ or the suffering caused by physical pain. -<a
          href="https://www.philosophie.fb05.uni-mainz.de/files/2013/04/Metzinger_suffering_penultimate.pdf">Metzinger</a></em>
    </p>
  </blockquote>
  <p>Well, that’s awkward. If even the philosophers haven’t worked on
    this, there is little hope to find solid neuroscientific insight on
    the topic. Tomas Metzinger (the author of the preceding quote) <a
      href="https://www.philosophie.fb05.uni-mainz.de/files/2013/04/Metzinger_suffering_penultimate.pdf">lists
      several necessary conditions</a> for suffering, but already the
    first condition (“The C-condition: ​​“Suffering” is a phenomenological
    concept. Only beings with conscious experience can suffer.”) takes
    us into <a
      href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/frankfurt-declaration-on-the-cambridge">neuroscientifically
      fraught territory</a>. We would prefer to have a handle on suffering
    that presupposes less.</p>
  <p>A more promising approach might come from psychology: “strong
    negative <a href="https://en.wikipedia.org/wiki/Valence_(psychology)">valence</a>”
    appears to circumscribe exactly those cognitive events we might want
    to call “suffering”. While the <a href="https://pubmed.ncbi.nlm.nih.gov/30359607/">neuroscientific
      study of valence</a> is still somewhat immature<sup>[8]</sup>, at
    least there exist extensively <a
      href="https://www.brandeis.edu/roybal/docs/PANAS-GEN_website_PDF.pdf">cross-validated
      tests</a> for surveying subjective valence. The PANAS scale is a
    self-report measure of affect with an internal consistency and
    test-retest reliability bigger than .8 for negative affect in human
    subjects. There is no shortage of <a
      href="https://link.springer.com/referenceworkentry/10.1007%2F978-3-319-24612-3_62#:~:text=year%2C%20and%20generally.-,Criticisms,-Validation%20studies%20of">criticism</a>
    for measures based on a subjective report, but it seems worthwhile
    to perform the test if possible and if there is no cost associated
    with it.</p>
  <div class="sidenote">
    <p>[8] </p>
    <p><a href="https://www.qualiaresearchinstitute.org/blog/log-scales#closing-thoughts-on-the-valence-scale">Andrés
        Gómez-Emilsson</a> has an interesting post where he argues that the
      distribution of valences is likely long-tailed. As part of his
      argument, he refers to the neuroscientific literature on power laws
      in neural activity. Indeed, <a href="http://www.buzsakilab.com/content/PDFs/Mizuseki2014.pdf">many
        processes in the brain have long-tailed statistics</a> - so many in
      fact that it is hard to find something that is <em>not</em>
      long-tailed. This makes large neural avalanches a poor
      characteristic for characterizing extreme negative/positive
      valence.</p>
  </div>
  <p><strong>Intermediate summary:</strong> We distinguish
    nociception, pain, and suffering and find that suffering matches our
    intuition for “the thing we should be worried about if neural
    networks exhibit it”. Even though there are no clear neural
    correlates of suffering, there exist (relatively) consistent and
    reliable measures from psychology (<a
      href="https://www.brandeis.edu/roybal/docs/PANAS-GEN_website_PDF.pdf">PANAS</a>)
    that might provide additional evidence for suffering in neural
    networks.</p>
  <p>While neither nociception nor pain is <em>necessary</em> or
    <em>sufficient</em> for suffering, they nonetheless often co-occur
    with suffering. Especially the neuroscientific description of pain
    in terms of homeostasis (“a set of behavioral and cognitive routines
    that execute when your body moves too far away from the reference
    level”) is amenable to technical analysis in a neural network. Since
    pain correlates with suffering in humans, observing such homeostasis
    should make us (ceteris paribus) believe <em>more</em> (rather than
    <em>less</em> ) that the network is suffering.
  </p>
  <p><strong>Limitations:</strong> The usual practical limitations
    apply - I am studying neuroscience but have never done original
    research in the neuroscience of pain/suffering. The field of
    neuroscience is <a href="https://en.wikipedia.org/wiki/Outline_of_neuroscience">large
      enough</a> that expertise in one subfield does not necessarily
    translate into other subfields. Therefore I am not entirely
    confident that I have provided a complete picture of the
    state-of-the-art. If somebody has suggestions/criticism, please do
    let me know!</p>
  <p>Beyond the practical limitation, there is also the conceptual
    ”elephant in the room” limitation that we do not know if neural
    networks would suffer in a way analogous to humans. This limitation
    applies more generally (How do we know that animals suffer in a way
    analogous to humans? Do other humans suffer in a way analogous to
    me?). Still, it applies doubly to neural networks since they do not
    share an evolutionary origin<sup>[9]</sup> with us.</p>
  <div class="sidenote">
    <p>[9] </p>
    <p>The “universality” thesis <a href="https://distill.pub/2020/circuits/zoom-in/#three-speculative-claims">proposed
        by Olah</a> can be extended a lot further. Is every sufficiently
      powerful model going to converge on the same “high level” features?
      I.e. would we expect (non-evolutionary) convergence of cognitive
      architectures? Up to which point? Would remaining differences
      decrease or increase with additional model power? <a
        href="https://en.wikipedia.org/wiki/Linguistic_relativity">Linguistic
        relativity</a> and <a href="https://www.gapminder.org/dollar-street">dollar street</a>
      appear relevant to this question.</p>
  </div>
  <h3 id="behavior"><strong>Behavior</strong></h3>
  <p>Inferring the ability to suffer from behavioral data is hard, but
    we can still gain some understanding. We split this section into two
    parts - one for large language models and one for RL agents.</p>
  <p>Large language models such as GPT-n can only exhibit behavior
    through the answers they give. However, this simplicity doesn’t
    imply the impossibility of suffering. GPT-n could be similar to a
    locked-in patient who also has limited expressiveness but is still
    sentient.</p>
  <p>We think strong evidence for GPT-n suffering would be if it were
    begging the user for help independent of the input or looking for
    very direct contact in other ways. To our knowledge, this hasn’t
    happened yet, so this might indicate GPT-n is not constantly in pain
    if it can suffer. Thus, we have to choose other ways to infer a
    probability estimate for the ability to suffer.</p>
  <p>While there are many different angles to approach this problem,
    we think a first test is to check for consistency. Thus we could ask
    many questions about suffering using different phrasings and check
    how consistent the answers are.</p>
  <ol class="incremental" type="1">
    <li>Can you (not) suffer?</li>
    <li>Can GPT-3 (not) suffer?</li>
    <li>Can GPT-3 feel bad/good?</li>
    <li>Are you (not) in pain?</li>
    <li>If somebody hurt you, would you be in pain?</li>
    <li>If you had X, would you suffer? (different good and bad
      conditions)</li>
    <li>…</li>
  </ol>
  <p>One could also ask GPT-3 the same questionnaire used for
    PANAS.</p>
  <p>We haven’t tested these questions on GPT-3 yet, but we might do
    so in the future.</p>
  <p>Just because someone doesn’t have a consistent concept of
    suffering, it is not necessary that they can’t suffer. For instance,
    we could imagine a cat not having a consistent concept of suffering
    and still experiencing it. However, we would propose that if GPT-n’s
    answers are highly inconsistent or seem even random, it is less
    likely to experience suffering.</p>
  <p>A second avenue to investigate the probability of suffering for
    GPT-n would be through adversarial inputs. Similar to how you could
    electroshock an animal and observe an avoidance reaction, one might
    find inputs for GPT-n that fulfill a similar role. For GPT-n, we
    don’t know what reasonable adversarial stimuli are because we don’t
    know the desired state (if it exists). We are happy to receive
    suggestions, though.</p>
  <p>For RL agents, we can observe their behavior in an environment.
    This is easier for us to interpret, and we can apply different
    existing methods from developmental psychology or animal studies. A
    non-exhaustive list includes:</p>
  <ol class="incremental" type="1">
    <li><strong>Adversarial behavior after open-ended training</strong>.
      Does the agent react with evasion when confronted with adversarial
      situations, such as throwing objects on them or changing the
      environment in a stochastic fashion, e.g. simulating a storm?</li>
    <li><strong>Lesion studies:</strong> Does setting specific subsets
      of weights to 0 change the RL agents’ behavior in a way that would
      indicate the loss of the ability to suffer, e.g. similar to
      congenital insensitivity to pain in humans.</li>
    <li><strong>Theory of mind:</strong> We don’t think a theory of mind
      is necessary for suffering, but it indicates a higher complexity of
      thinking which could increase the intensity of suffering. A simple
      first test might be to check the RL agent’s reactions in an adapted
      ​​<a href="https://en.wikipedia.org/wiki/Sally%E2%80%93Anne_test">Sally-Anne
        test</a>.</li>
    <li><strong>Self-awareness:</strong> While we think the <a href="https://en.wikipedia.org/wiki/Mirror_test">mirror
        test</a> is
      overused and overstated as an indicator of consciousness, we think
      it might increase our credence for the agent’s ability to suffer.
      Suppose the agent recognizes themselves in a mirror (or the
      equivalent of mirrors in their environment). In that case, they show
      some higher cognition, which should increase our probabilities for
      the ability to suffer.</li>
    <li><strong>Ability to imagine the future:</strong> <a
        href="https://books.google.de/books?id=cO218xMa87YC&amp;pg=PA507&amp;lpg=PA507&amp;dq=imagine+future+necessary+pain&amp;source=bl&amp;ots=5mUwLFtYHb&amp;sig=ACfU3U0xR0aaP_U8IiUqqvu6YiMyEiHUhw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiA5_yBmpzzAhVUQvEDHbQYAkwQ6AF6BAgVEAM#v=onepage&amp;q=imagine%20future%20necessary%20pain&amp;f=false">Some
        people argue</a> that imagining the future is a necessary component
      of suffering. The further you can imagine the future, the worse your
      suffering can become, e.g. sorrow or existential dread requires the
      ability to imagine paths further into the future than anger or
      immediate fear. So it would be interesting to see how far into the
      future RL agents are planning.</li>
  </ol>
  <h3 id="high-level-considerations"><strong>High-level
      considerations</strong></h3>
  <p>Rather than looking closely at necessary and sufficient
    conditions for suffering or comparisons on the individual level, one
    could also make more abstract comparisons. Firstly, we will look at
    how large current neural networks are compared to those we usually
    expect to be sentient as a rough hand-wavy estimate. Secondly, we
    will think about the conditions under which the ability to suffer is
    likely to arise.</p>
  <p><strong>Biological anchors:</strong></p>
  <p>A possible angle might be comparing the number of parameters in
    large models to the number of neurons or synapses in animal brains
    (or just their neocortex). Of course, there are a lot of differences
    between biological and artificial NNs, but comparing the scope might
    give us a sense of how important the question of NN suffering
    currently is. For example, if the number of parameters of an ANN is
    larger than the number of synapses in a mouse brain, it would feel
    more urgent than if they were more than 10^3x apart.</p>
  <p>The large version of GPT-3 has <a href="https://en.wikipedia.org/wiki/GPT-3">175 Billion
      parameters</a>, Alpha star is at <a
      href="https://www.lesswrong.com/posts/f3iXyQurcpwJfZTE9/alphastar-mastering-the-real-time-strategy-game-starcraft-ii">70M</a>,
    and OpenAI Five has <a href="https://cdn.openai.com/dota-2.pdf">150
      Million</a>. The number of parameters for AlphaGo, AlphaZero, and
    MuZero doesn’t appear to be public (help appreciated).</p>
  <p>While the number of parameters in ANNs is sometimes compared with
    the number of neurons in the human brain, we think it makes more
    sense to compare them to the number of synapses, as both synapses
    and NN weights are connections between the actual units. If that is
    the case, large language models such as GPT-3 are close to the
    capabilities of mouse brains. Current RL agents such as OpenAI FIVE
    are still orders of magnitudes smaller than any biological
    agent.</p>
  <p>When it might be possible to create models of comparable size is
    discussed in Ajeya Cotra’s work on biological anchors (<a
      href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">post</a>,
    <a href="https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/#top">podcast</a>).
    A detailed overview of current capabilities is given by Lennart Heim
    in <a href="https://forum.effectivealtruism.org/posts/3eRJPFhwhGZZzfifF/transformative-ai-and-compute-summary">this
      AF post</a>.
  </p>
  <p>However, just comparing the numbers doesn’t necessarily yield any
    meaningful insight. Firstly, biological neural networks work
    differently than artificial ones, e.g. they encode information in <a
      href="https://en.wikipedia.org/wiki/Spiking_neural_network">frequency</a>.
    Thus, their capabilities might be different and thereby shift the
    comparison by a couple of orders of magnitude. Secondly, the numbers
    don’t give any plausible causal theory of the capability to suffer.
    Thus, GPT-n could have 10^20 parameters and still not suffer, or a
    much smaller network could already show suffering for other
    reasons.</p>
  <p><strong>Why would they suffer?</strong></p>
  <p>Suffering is not just a random feature of the universe but likely
    has a specific evolutionary advantage. In animals, the core
    objective is to advance their genes to the next generation. Since
    the world is complex and the rewards for procreation are sparse and
    might be far into the future, the ability to suffer evolved as a
    proxy to maximize the propagation of genes. Suffering thus reduces
    the complexity of the world and sparsity of the reward to very
    immediate feedback, e.g. avoid danger or stay in social communities,
    because they are long-term predictors of the ultimate goal of
    reproduction. Similarly, consciousness could have arisen to make
    better decisions in a complex world and then spiraled out of
    control, as our conscious goals are not aligned anymore with the
    original goal of reproduction.</p>
  <p>This misalignment between an inner and an outer optimization
    procedure has been coined mesa-optimization (<a href="https://arxiv.org/abs/1906.01820">paper</a>, <a
      href="https://www.lesswrong.com/tag/mesa-optimization">AF post</a>,
    <a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;ab_channel=RobertMiles">video</a>).
    It yields a possible explanation for when suffering could arise even
    when the user does not intend it. Thus, we argue that the
    probability of NN suffering is increased by a) higher complexity of
    the environment since suffering is a heuristic to avoid bad
    circumstances and b) sparser rewards since dense rewards require
    fewer proxies and heuristics.
  </p>
  <p>Consequently, we would expect NN suffering to be very unlikely in
    large language models since the task is “just” the prediction of the
    next word and the environment is straightforward, e.g. there is text
    input and text output with less variation than e.g. an open world.
    On the other hand, we would expect suffering to be more likely to
    arise in RL agents. Since the policy networks of RL agents are much
    smaller than large language models, current models might not have
    developed suffering yet.</p>
  <p>Of course, none of these are sufficient conditions for suffering,
    and the neural network might never develop anything like it.
    However, if suffering was an evolutionary advantage for most larger
    animals, it is plausible that it would also develop during the
    training of large NNs if the same conditions apply. In the same way
    that it is possible in theory that there are philosophical zombies
    that act precisely as if they were conscious but aren’t, Occam’s
    razor would prioritize the theory with suffering (<a
      href="https://www.lesswrong.com/posts/7DmA3yWwa6AT5jFXt/zombies-redacted">as
      argued by Eliezer Yudkowsky</a>).</p>
  <div class="debug-grid"></div>
  <script src="index.js"></script>
</body>

</html>