<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="" >

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="Jan Kirchner" />
      <meta name="dcterms.date" content="2021-11-22" />
        <title>Slightly advanced decision theory 102 Four reasons not to be a (naive) utility maximizer</title>
    <link rel="stylesheet" href="../../reset.css" />
    <link rel="stylesheet" href="../../index.css" />
      </head>

<body>
    <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title"><a href="https://kirchner-jan.github.io/minimalprior/"
            style="text-decoration: none; color: inherit;">Slightly
advanced decision theory 102 Four reasons not to be a (naive) utility
maximizer</a></h1>
        <span class="subtitle">TL;DR Introduction to decision theory
with examples from effective altruism. Resulting optimal strategies
differ from “naive” utility maximisation, but…</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">2021-11-22</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://kirchner-jan.github.io/minimalprior/">Jan
Kirchner</a></td>
    </tr>
  </table>
      <nav id="TOC" role="doc-toc">
        <ul class="incremental">
        <li><a href="#foundations-and-flaws"
        id="toc-foundations-and-flaws">Foundations and Flaws</a></li>
        <li><a href="#average-action"
        id="toc-average-action"><strong>Average action</strong></a></li>
        <li><a href="#suboptimal-solutions"
        id="toc-suboptimal-solutions"><strong>Suboptimal
        solutions</strong></a></li>
        </ul>
  </nav>
    <p><em>Hello and welcome to everyone who joined from Hacker News or
    Marginal Revolution after last week’s post</em><sup>[1]</sup><em>!
    I’m Jan and I’m trying to have something interesting to say on this
    Substack every week. Quality and mass appeal can vary. Hope you
    enjoy &lt;3</em></p>
    <div class="sidenote">
    <p>[1] </p>
    <p>The post got more than 20.000 (pairs of) eyeballs! Gasp!</p>
    </div>
    <hr />
    <h2 id="foundations-and-flaws">Foundations and Flaws</h2>
    <p>In high school I learned that there are four fundamental
    questions in philosophy:</p>
    <ul class="incremental">
    <li><p><em>What can I know?</em> (<a
    href="https://plato.stanford.edu/entries/epistemology/">epistemology</a>)</p></li>
    <li><p><em>What should I do?</em> (<a
    href="https://iep.utm.edu/ethics/">moral philosophy</a>)</p></li>
    <li><p><em>What is beauty?</em> (<a
    href="https://iep.utm.edu/aestheti/">aesthetics</a>)</p></li>
    <li><p><em>What?</em> (<a
    href="https://plato.stanford.edu/entries/metaphysics/">metaphysics</a>)</p></li>
    </ul>
    <p><a href="https://www.effectivealtruism.org/">Effective
    altruism</a> is a movement that attempts to link the second question
    with the first<sup>[2]</sup><sup>[3]</sup>: <em>How can I use the
    things I know to better do the things I should do?</em></p>
    <div class="sidenote">
    <p>[2] </p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_8.gif" /></p>
    </div>
    <div class="sidenote">
    <p>[3] </p>
    <p>Note how “<a
    href="https://threadreaderapp.com/thread/1206339190352437255.html">postrats</a>”
    are focused on questions three and four instead.</p>
    </div>
    <p>This leads to an interesting clash of cultures: people interested
    in “what can I know?” tend to be rather cerebral<sup>[4]</sup>,
    while “what should I do?” attracts people that are more…
    cordial?<sup>[5]</sup> This mix is often <a
    href="https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks">intriguing</a>
    and sometimes <a
    href="https://mindingourway.com/the-value-of-a-life/">sublime</a>.
    It’s a young movement and there are many <a
    href="https://globalprioritiesinstitute.org/">foundational
    questions</a> that need figuring out. In a [previous post], I
    promised that I would dig into the foundations and see if everything
    <em>actually</em> makes sense to me. This post is the first step
    down that path.</p>
    <div class="sidenote">
    <p>[4] </p>
    <p>Or “<a
    href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/soldiers-scouts-and-albatrosses">scoutish</a>”.</p>
    </div>
    <div class="sidenote">
    <p>[5] </p>
    <p>I guess that is the appropriate opposite of cerebral. Less
    charitable, they can also be called<a
    href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/soldiers-scouts-and-albatrosses">soldiers</a>.</p>
    </div>
    <p>So buckle up, I’ll be diving into expected value reasoning and
    slightly advanced decision theory. In four bite-sized sections that
    can be enjoyed in sequence or in isolation, I’ll provide theoretical
    footing for a lot of the things that are already practiced in
    effective altruism anyway, but that do not follow directly from
    naively applying expected value reasoning. As mentioned above, I’m
    partially doing this to satisfy my own need for having a solid
    theoretical basis for my actions. But beyond that, my hope is also
    that by articulating these arguments and demonstrating how to use
    computational toy models to understand complicated dynamics I’m
    providing useful tools for the community.</p>
    <h2 id="average-action"><strong>Average action</strong></h2>
    <p>But before we can talk about tools, let me set the stage. Feel
    free to skip this section if you are familiar with the central logic
    and recommendations for action of effective altruism.</p>
    <p>I said in the earlier section that effective altruism combines
    epistemology with moral philosophy. What does that even
    mean<sup>[6]</sup>?</p>
    <div class="sidenote">
    <p>[6] </p>
    <p>This is a tough problem in philosophy. David Hume says it’s
    impossible, see <a
    href="https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem">Hume’s
    Guillotine</a>.</p>
    </div>
    <p>We can make this <a
    href="https://www.williammacaskill.com/info-moral-uncertainty">arbitrarily
    complicated</a>, but for the sake of the post let us start by
    picking <a
    href="https://iep.utm.edu/hedonism/#:~:text=f.%20Hedonistic%20Utilitarianism">hedonistic
    utilitarianism</a> as moral philosophy. Hedonistic utilitarianism
    prescribes that we ought to take the action that produces the
    greatest net happiness for all concerned. The word “greatest”
    suggests that we need to be able to <a
    href="https://en.wikipedia.org/wiki/Greatest_element">order all
    actions so that one of them is better than all the other ones</a>.
    At this point, the <a
    href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">Von-Neumann-Morgenstern</a>
    is usually introduced to argue that under certain “reasonable
    assumptions” we can get a utility function from an ordering of
    preferences. The prescription of hedonistic utilitarianism to
    “produce the greatest net happiness for all concerned” then
    translates into “choose actions that maximize utility for all
    concerned”. And figuring out which action that is, turns out to be
    something that we (<a
    href="https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">sometimes,
    kind of</a>) can do by looking at the world and figuring out how
    things work. Thus we arrive in the land of epistemology.</p>
    <p>So far, so vague - what does this buy us? It turns out that it is
    pretty hard to <em>actually</em> apply
    Von-Neumann-Morgenstern<sup>[7]</sup>, therefore the idea to
    maximize utility is more like a guiding light than an ultimate law.
    Reality is often messy and we cannot be sure that the action we take
    ends up having the effect we think they will have. Even the most
    promising action can fail - and an unlikely “moonshot” project might
    still end up working out. This probabilistic component leads us to
    <a
    href="https://reducing-suffering.org/why-maximize-expected-value/">maximize
    the</a><em><a
    href="https://reducing-suffering.org/why-maximize-expected-value/">expected</a></em><a
    href="https://reducing-suffering.org/why-maximize-expected-value/">utility</a>
    instead of just maximizing utility. This gives us some assurance
    that as long as we keep trying long enough, eventually, we will
    produce utility.</p>
    <div class="sidenote">
    <p>[7] </p>
    <p>And hedonistic utilitarianism has some<a
    href="https://en.wikipedia.org/wiki/Wirehead_%28science_fiction%29">nasty
    pathological solutions</a>.</p>
    </div>
    <p>Despite these caveats, a handful of recommendations have emerged
    for the aspiring effective altruist that have a chance to maximize
    expected utility:</p>
    <ol class="incremental" type="1">
    <li>Donate. You are <a
    href="https://howrichami.givingwhatwecan.org/how-rich-am-i">very
    likely among the richest five percent of people</a> living on the
    planet right now. Your donation can improve someone else’s life
    tremendously.</li>
    <li>Donate to an <em>effective</em> charity. Some charities are <a
    href="https://www.givingwhatwecan.org/charity-comparisons/">up to
    100x more cost-effective</a> than other charities. The same amount
    of money donated might therefore produce up to 100x the expected
    utility.</li>
    <li>Choose an effective career. If you work on a problem that is ‘<a
    href="https://80000hours.org/articles/your-choice-of-problem-is-crucial/">important,
    neglected, and tractable</a>’, you have a chance of working on
    something with high expected utility (important) that not enough
    people are working on (neglected) and that you might
    <em>actually</em> solve (tractability).</li>
    <li>Consider being risk-neutral rather than risk-averse. Our natural
    instinct is to be <a
    href="https://en.wikipedia.org/wiki/Allais_paradox">risk-averse</a>
    and not to take options that are good in expected value but have a
    high chance of failure. <a
    href="https://80000hours.org/articles/risk/">Correcting this bias
    might unlock a lot of additional expected utility</a>.</li>
    </ol>
    <h2 id="suboptimal-solutions"><strong>Suboptimal
    solutions</strong></h2>
    <p>Are you convinced? I hope not. Here are three points that give me
    pause:</p>
    <ol class="incremental" type="1">
    <li><strong>Career choice</strong>. I <a
    href="https://80000hours.org/speak-with-us/?int_campaign=2021-08__primary-navigation#:~:text=We%E2%80%99ve%20helped%20over%201%2C000%20people">often
    meet effective altruists</a> who struggle hard to make “the most
    important decision of their life” regarding which field to study.
    The intersection between personal fit, uncertain future, and (de
    facto) incommensurability of different cause areas makes this a
    devilishly hard question. But - from personal experience - this
    question also mostly turns out to be a <a
    href="https://en.wikipedia.org/wiki/False_dilemma">false
    dilemma</a>. You <em>can</em> have your cake and eat it too.
    Sometimes, at least.</li>
    <li><strong>Risk-neutrality</strong>. The point about
    risk-neutrality doesn’t sit well with me at all. Call me
    risk-averse, but I can’t argue myself into dropping my Ph.D. and
    starting a start-up<sup>[8]</sup>. I’m all for <a
    href="https://en.wikipedia.org/wiki/Skin_in_the_Game_%28book%29">skin
    in the game</a> and I can hear Taleb calling me an <a
    href="https://medium.com/incerto/the-intellectual-yet-idiot-13211e2d0577">IYI</a>,
    but something about the logic doesn’t sit well with me. Almost as if
    there is a hidden risk that doesn’t go into the expected value
    calculation…
    <div class="sidenote">
    <p>[8] </p>
    <p>Maybe later though… * <em>stares off into the distance</em> *</p>
    </div></li>
    <li><strong>Donation splitting</strong>. Expected utility reasoning
    implies that some cause areas are more impactful than others.
    Perhaps we can’t figure out which ones they are, but if we
    <em>could</em> , then, according to naive expected value reasoning,
    we should <a
    href="https://funds.effectivealtruism.org/help/20ApN8qGQIW8OS02KOA0WA">completely
    discard</a> the others. This is clearly nonsense to me - and to the
    majority of effective altruists, I believe. If we found out that <a
    href="https://reducing-suffering.org/the-importance-of-insect-suffering/">insect
    suffering</a> turns out to be really important, would we just drop
    everything else and focus on insects?</li>
    </ol>
    <p>Let’s see if we can untangle this and see where the discord is
    coming from.</p>
    <h4 id="diverse-detours."><strong>Diverse detours.</strong></h4>
    <p>On the topic of career choice: I have the impression that<a
    href="https://80000hours.org/about/#:~:text=your%20choice%20of%20career%20is%20probably%20your%20best%20opportunity%20to%20do%20that">too
    much importance is assigned to the choice of a career</a>. 80.000
    hours writes:</p>
    <blockquote>
    <p>If you want to have a positive impact with your life, your choice
    of career is probably your best opportunity to do that.</p>
    </blockquote>
    <p>Framing the situation like this makes it sound like a monolithic
    decision that locks in your future impact. I’m sure this is not the
    intended message; but from talking to people at the beginning of
    their careers, this is often the impression I get.</p>
    <p>An alternative approach was recently <a
    href="https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists">outlined
    by Holden Karnofsky</a>. Rather than monolithic paths into a career,
    Holden talks about aptitudes, which “one can build in a wide variety
    of roles and causes”. This matches my own perspective; I’ve
    consistently found it more beneficial to collect skills and friends
    as they present themselves to me, rather than to stubbornly grind
    towards a fixed goal. I’ve also made it a habit to say “yes” to
    <em>any</em> interesting opportunity that comes
    up<sup>[9]</sup>.</p>
    <div class="sidenote">
    <p>[9] </p>
    <p>A long time ago (long before I started to obsessively note down
    sources for things I hear) I heard this terrifying story about how
    actors in Hollywood never get <em>told</em> that their career is
    over. At some point, the phone just stops ringing.</p>
    </div>
    <p>So what can I add that’s not already in Holden’s post? A
    mathematical argument, of course!</p>
    <p>Let’s first look at one example of what a monolithic life
    decision might look like. Imagine you are evaluating several options
    (from <a
    href="https://www.effectivealtruism.org/articles/biosecurity-as-an-ea-cause-area-claire-zabel/">biorisk</a>
    research to <a
    href="https://forum.effectivealtruism.org/tag/cause-x">cause X</a>
    research). Each of them has an expected impact and you compare them
    to identify the winner:</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_129.png" /></p>
    <p>Since “cause X research” has the highest expected impact, you
    decide to pick that and drop the rest. In equations, you have
    opportunities</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_130.png" /></p>
    <p>and you decide to pick the one that has the highest expected
    value:</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_131.png" /></p>
    <p>While this is fine, and certainly has a higher impact than
    exclusively focusing on knitting, I believe we can do even better.
    In particular, the <em>expected value of the maximum is larger than
    or equal to the maximum of the expected values</em> :</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_132.png" /></p>
    <p>I know this mathematical theorem as the “<a
    href="https://link.springer.com/article/10.3758/s13414-015-1018-y">race
    model inequality</a>”<sup>[10]</sup>, but I’m sure it is known under
    different names in different fields. What it cashes out to is the
    following decision rule: When you have the choice between (ex-ante)
    picking one thing and getting the result from that thing, or
    (ex-post) seeing the result from all the things and picking the one
    you like best, you should always pick the latter (ex-post)
    option.</p>
    <div class="sidenote">
    <p>[10] </p>
    <p>I really hope this particular combination of terms doesn’t get
    picked up by the search engines.</p>
    </div>
    <p>Or, in less fancy terms,</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_6.jpeg" /></p>
    <p>Instead of <em>only</em> researching Cause X, also keep one foot
    in AI safety research, have a reading group on policy, and <a
    href="https://en.wikipedia.org/wiki/Arachne">knit as if your life
    depends on it</a>. Since there is variance in your actual outcome
    (you might end up being a terrible researcher, but your knitting
    skills inspire the countries of the world to unite and sing praise),
    it is better to hedge your bets and try multiple things.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_133.png" />
    <strong>a</strong> Ex-post outcome (do all the things and pick the
    thing that worked best) compared with ex-ante outcome (only do the
    one thing with the highest expected value. <strong>b</strong>
    Average ex-ante and ex-post outcomes. Code is <a
    href="https://colab.research.google.com/drive/1enMNClWabFr2KhmMOe1mRSP9MkVsU6kx?usp=sharing">here</a>.</p>
    <p>One (reasonable) objection might be that I’m neglecting an
    important limiting factor: humans can’t do <em>all the things</em>.
    If you try to do five things at the same time, they might, in
    combination, end up worse than if you had done one thing
    “properly”.</p>
    <p>But whoever says this underestimates <a
    href="https://en.wikipedia.org/wiki/Extreme_value_theory">the power
    of the</a><em><a
    href="https://en.wikipedia.org/wiki/Extreme_value_theory">maximum</a></em>.
    We can cut the expected value of all our opportunities in half
    (since we will not be able to do everything “properly”) and still
    take almost no hit to the ex-post expected value.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_134.png" /></p>
    <p>There are only two conditions: We should focus on opportunities
    that have at least broadly comparable expected values and that have
    a big amount of variability<sup>[11]</sup>. Doing a zany coding
    project with your friends that probably won’t work out but will
    revolutionize how we wear our socks if it does? Go for it! Read a
    textbook from a field that sounds extremely relevant to your work
    but that none of your colleagues cares about? Absolu-telly. Go to
    that weird party that you were totally invited to by accident and
    that you will very likely leave a little embarrassed? Yep, even
    that.</p>
    <div class="sidenote">
    <p>[11] </p>
    <p>Well, technically there is a whole cartload of<a
    href="https://en.wikipedia.org/wiki/Extreme_value_theory">caveats
    and special cases</a>, and I haven’t even talked about long tails.
    But I think the general argument is clear.</p>
    </div>
    <p>“Maximize variance”, a friend once told me. Those are words to
    live by.</p>
    <h4 id="intoxicated-imagination."><strong>Intoxicated
    imagination.</strong></h4>
    <p>(For comedic effect I’m putting these sections back to back.)</p>
    <p>Stay. Away. From. Variability.</p>
    <p>While 80000 hours inconspicuously whispers into my ear about the
    <a
    href="https://80000hours.org/career-reviews/tech-entrepreneurship/">potentially
    very large impact of a tech startup founder</a>,
    AppliedDivinityStudy shouts at me to<a
    href="https://applieddivinitystudies.com/billionaire/">become a
    billionaire already</a>. While unlikely to work, <em>if</em> it
    does, it’ll be awesome. High risk, high gain. Sure, <em>you</em>
    will likely fail, but if enough people try, <a
    href="https://fortune.com/2021/07/29/sam-bankman-fried-crypto-billionaire-ftx/">someone
    is bound to succeed</a>.</p>
    <p>I don’t even want to push back on this too strongly. There are <a
    href="https://d-nb.info/1152136895/34">strong incentives that
    contribute to the risk-averseness of academics</a> and I try to stay
    aware of my biases. However, here is an important concept that is
    often ignored when high-risk opportunities are advertised: <a
    href="http://ce.sharif.edu/courses/90-91/1/ce695-1/resources/root/Notes/Lec-8b.pdf">absorbing
    states</a>.</p>
    <blockquote>
    <p>An absorbing state is one in which the probability that the
    process remains in that state once it enters the state is 1.</p>
    </blockquote>
    <p>The most famous absorbing state is death. Less permanent
    absorbing states are imprisonment, <a
    href="https://en.wikipedia.org/wiki/Persistent_vegetative_state">persistent
    vegetative states,</a> and cult membership. There are also
    pseudo-absorbing states, like <a
    href="https://www.nolo.com/legal-encyclopedia/starting-business-after-bankruptcy.html">bankruptcy</a>,
    <a
    href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5769037/">depression</a>,
    and <a
    href="https://www.thebalance.com/hard-to-pay-off-debt-960855">debt</a>.<sup>[12]</sup>
    If your amazing high-risk opportunity puts you in danger of ending
    up in one of those states, say ” <em>No thank you</em> ” and put
    some distance between you and the opportunity.</p>
    <div class="sidenote">
    <p>[12] </p>
    <p>I guess none of those are <em>technically</em> absorbing states
    as the probability to remain in the state is not 1. But this is
    <em>my</em> Substack, therefore I get to make
    technically-not-quite-correct statements.</p>
    </div>
    <p>Joke aside, there are certain <a
    href="https://www.aljazeera.com/news/2021/10/16/seven-hong-kong-activists-jailed-over-unauthorised-protest-in-2020">situations
    when it is okay to risk imprisonment</a>. But I want to argue that
    absorbing boundaries should not be underestimated. Here is another
    toy model:</p>
    <p>Let’s say you get offered a series of opportunities with positive
    expected value, but high variability. If you accept sufficiently
    many of them, you are almost guaranteed to end up with a large
    positive payoff. In the process of getting there, you will
    experience substantial fluctuations, but they will cancel out and
    the positive expected value of the opportunity will accrue.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_135.png" />
    <strong>a</strong> Value of the opportunity as a function of time.
    Multiple equally distributed time series are shown. The dashed line
    in red indicates zero value. <strong>b</strong> Histogram plot of
    payoffs (value of opportunity at time point 100). The dashed line in
    red indicates average payoff. Code is <a
    href="https://colab.research.google.com/drive/1b2xENV91mZfsf_BzMGufpGdl5-ldeTMW?usp=sharing">here</a>.</p>
    <p>However, things look very different when we introduce an
    absorbing boundary at zero. Due to the low start value and the large
    variability, almost all instantiations of the scenario end up in the
    absorbing boundary (bankrupt, dead, or worse)</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_136.png" />As
    the previous figure, but with all opportunities that hit zero
    clamped to zero.</p>
    <p>So what does that mean? Should we avoid opportunities with high
    variability under all circumstances?</p>
    <p>No, in particular in light of my argument from the previous
    section this would be too extreme. I’m only arguing that we need to
    be aware of the existence of absorbing boundaries. When they exist,
    you should still go for a high expected value - but additionally,
    you might want to try and minimize variability.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_137.png" />Expected
    payoff for different combinations of expected value and variance of
    the driving noise process. Colorbar is on a log scale. Note how a
    medium-sized expected value with low variance might result in a
    higher payoff compared to a high expected value with high
    variance.</p>
    <p>(There is another fun little insight that we get from this toy
    model. Notice how in the left half of the figure variance has a
    <em>positive</em> effect on payoff? This makes sense - if you have
    very little to gain and will likely end up in the absorbing state
    anyways then it pays to strive for highly variable outcomes. You can
    only win. So in that spirit: Ask that woman/man/creature you like
    out on a date. You can only win.<sup>[13]</sup>)</p>
    <div class="sidenote">
    <p>[13] </p>
    <p>This little excursion channeled my inner <a
    href="https://putanumonit.com/">Putanumonit</a> for a second.</p>
    </div>
    <hr />
    <p>Now we turn to the last point on my list: the unintuitive notion
    that you<a
    href="https://reducing-suffering.org/why-maximize-expected-value/">ought
    to discard</a> any charitable cause that has less expected impact
    than the other causes. I’m afraid this sounds like a <a
    href="https://en.wikipedia.org/wiki/Straw_man">straw man</a> since
    nobody in the community actually acts like this<sup>[14]</sup>.
    Somehow it is implicitly acknowledged that there is no “one true
    charity” with the highest expected impact and if we only knew which
    one it is, then we’d just scrap the rest. But still, there are <a
    href="https://reducing-suffering.org/why-maximize-expected-value/#What_about_mixed_strategies">several</a><a
    href="https://funds.effectivealtruism.org/help/20ApN8qGQIW8OS02KOA0WA">posts</a>
    out there arguing against donation splitting. And, anecdotally, I
    can confirm that some EAs are a bit confused about why that should
    apply to them personally but not to the institutions at large.</p>
    <div class="sidenote">
    <p>[14] </p>
    <p>Except perhaps in the context of the latent rift between
    “traditional EA” (global health &amp; poverty, animal suffering) and
    “fancy EA” (longtermism, AI safety).</p>
    </div>
    <p>In the next two sections, I’ll present two more arguments for why
    it’s (sometimes) fine to spread your efforts across multiple
    opportunities.</p>
    <h4 id="gotta-catch-them-all."><strong>Gotta catch them
    all.</strong></h4>
    <p>Imagine you are caught in a psychology student’s nightmare and
    have to choose between two sources of reward. One provides rewards
    on 70% of occasions where you approach it, the other only on 30%.
    The standard answer from decision theory is that you ought to always
    go to the first source<sup>[15]</sup>.</p>
    <div class="sidenote">
    <p>[15] </p>
    <p>Quick demonstration:</p>
    </div>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_138.png" />
    <strong>a</strong> Likelihood of success (fraction of trials where
    you guess correctly) as a function of the matched probability
    (fraction of trials where you pick source one). Different colors
    denote different “true biases” (the actual fraction of trials where
    the reward is at source one). Red arrows indicate the optimal choice
    of matched probability for different underlying true biases.
    <strong>b</strong> Optimal matched probabilities (red arrows in
    <strong>a</strong> ) as a function of the true bias. Code is <a
    href="https://colab.research.google.com/drive/1Soj_HJ6XZZm84-clCGoKRfRCOvoZUGnv?usp=sharing">here</a>.</p>
    <p>(This is basically the argument for why you <a
    href="https://reducing-suffering.org/why-maximize-expected-value/#What_about_mixed_strategies">should
    not split your donations</a>. Splitting your donations, or
    alternating between the sources, <em>will</em> reduce your expected
    utility.)</p>
    <p>However, humans <em>love</em> to mix things up. This behavior is
    called is <a
    href="http://naturalrationality.blogspot.com/2007/11/probability-matching-brief-intro.html">probability
    matching</a>,</p>
    <blockquote>
    <p>a widely observed phenomenon in which subjects match the
    probability of choices with the probability of reward in a
    stochastic context.</p>
    </blockquote>
    <p>In the example from the psychology student’s nightmare,
    probability matching means that 30% of the time you would go for one
    source and 70% of the time, you go for the other. This behavior is
    suboptimal from the perspective of maximizing expected
    reward<sup>[16]</sup>, but humans <em>still</em> do it all the time.
    There is an <a
    href="https://zenodo.org/record/1067055/files/article.pdf">ongoing
    debate</a> about what to make of this (are humans dumb or just extra
    smart?), but I actually would like to focus on one aspect of the
    problem that hasn’t been explored extensively yet. In the game with
    the two sources of reward, it’s implicitly assumed that the two
    kinds of reward are <a
    href="https://en.wikipedia.org/wiki/Fungibility">fungible</a>,
    i.e. that they are interchangeable in the same way that money is
    interchangeable. Having a reward from <em>only</em> one of the
    sources is acceptable.</p>
    <div class="sidenote">
    <p>[16] </p>
    <p>Resulting in only <r> = 2pp-2p + 1 expected reward.</p>
    </div>
    <p>What happens when we make things… <a
    href="https://www.thisworldthesedays.com/source28.html">non-fungible</a>?
    That is, what happens if you definitely need <em>at least one</em>
    from either source? You can come up with your own favorite example
    here<sup>[17]</sup>, but I like to think about <a
    href="https://theprecipice.com/">existential risk</a>. If there is
    an asteroid about to impact the earth <em>and</em> a supervolcano
    about to erupt, then it doesn’t make sense to ask to pick either
    <em>or</em> the other. In that situation it is pretty clear that to
    achieve your goal, you have to divide your efforts.</p>
    <div class="sidenote">
    <p>[17] </p>
    <p>Happiness and love, wisdom and beauty, a monster truck, and a<a
    href="https://advancedgraphics.com/rocky-from-rocky-ii-cardboard-cutout-2786/">Rocky
    from Rock II cardboard cutout</a></p>
    </div>
    <p>Divide them how? Turns out, you pretty much have to do inverse
    probability matching.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_139.png" />Same
    as the previous figure, but with “success” defined as “getting at
    least one reward from both sources”. The dashed line in
    <strong>b</strong> indicates one minus the identity. Code is <a
    href="https://colab.research.google.com/drive/1CUKhnxJi9BJVSUS95ZeYV0Yv7rkKkBex?usp=sharing">here</a>.</p>
    <p>As you need a reward from <em>both</em> sources, you ought to
    frequently visit the source that rarely provides reward to make sure
    that you end up getting it when it shows up. In contrast, you can
    rarely visit the source that often provides reward, since on those
    rare occasions you are guaranteed to pick up the reward.</p>
    <p>If we translate the toy model into the domain of existential risk
    prevention, then the model predicts that you should allocate your
    resources to existential risk prevention causes in proportion to how
    hard the risk is to prevent. I’m pretty sure this way of thinking is
    related to <a
    href="https://www.existential-risk.org/concept.pdf">Nick Bostrom’s
    Maxipok rule</a>:</p>
    <blockquote>
    <p>Maximise the probability of an ‘OK outcome’, where an OK outcome
    is any outcome that avoids existential catastrophe.</p>
    </blockquote>
    <p>Under the maxipok rule, we don’t care about whether we get an
    unholy amount of reward from one of the sources, we only care about
    making sure that nothing <em>terrible</em> flies under the
    radar.</p>
    <h4 id="the-first-and-the-tenth-oreo-cookie."><strong>The first and
    the tenth Oreo cookie.</strong></h4>
    <p>While the argument from non-fungible goods is already sufficient
    to justify splitting donations across existential risk prevention
    charities, no discussion of the topic would be complete without
    mentioning <a
    href="https://forum.effectivealtruism.org/tag/diminishing-returns">diminishing
    returns</a>. I expect that most people are familiar with the
    concept, but I’m having a <em>lot</em> of fun with my toy models and
    with making pretty figures, so here we go.</p>
    <p>More of a good thing is not always more good. Utilities are
    context-sensitive and can depend on time. The first Oreo cookie is
    great, the tenth is kind of good, the one hundredth <em>will</em>
    make you sick. In your hypothetical decision theory 101
    class<sup>[18]</sup>, you typically consider the following simple
    <em>linear</em> model that links inputs <strong>x</strong> to
    expected returns <strong>r</strong> :</p>
    <div class="sidenote">
    <p>[18] </p>
    <p>We skipped that one since it’s much more interesting to write
    about advanced stuff.</p>
    </div>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_140.png" /></p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_141.png" />Expected
    return as a function of income. Different colored lines indicate
    different opportunities with different scaling factors “a”. The
    dashed line indicates the identity. Code is <a
    href="https://colab.research.google.com/drive/1cz5PLAwbKEaBP-bOs82jzvI0NU2eLJ7I?usp=sharing">here</a>.</p>
    <p><em>Linear</em> means that if you invest twice as much input, you
    will get exactly twice as much output. This assumption is justified
    when we are talking about “<a
    href="https://en.wikipedia.org/wiki/Taylor_series">relatively
    small</a> <strong>x</strong>”.</p>
    <p>Let’s say we have different opportunities</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_142.png" /></p>
    <p>, but only a limited budget (time, money, motivation). How should
    we split our budget across the different opportunities?</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_143.png" /></p>
    <p>In this case, we can derive that the optimal choice is to go
    <em>all-in</em> on the most promising opportunity, i.e. the one that
    has the highest scaling factor
    <strong>a</strong><sup>[19]</sup>.</p>
    <div class="sidenote">
    <p>[19] </p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_150.png" /></p>
    </div>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_144.png" />Same
    as the previous figure, but now with red arrows indicating the
    optimal split of budget to different opportunities. Note how only
    one opportunity (the one with the largest scaling factor
    <strong>a</strong> ) has a nonzero amount of budget allocated.</p>
    <p>When charitable donations in EA consisted of <a
    href="https://harvardpolitics.com/new-social-movement-generation-effective-altruism/#:~:text=Givewell%20co%2Dfounder%20Toby%20Ord%20donated%20ten%20percent%20of%20his%20income%20while%20living%20on%20a%20graduate%20student%E2%80%99s%20salary%20at%20Oxford">a
    few graduate students giving away their spare change</a>, the linear
    model applied and dictated that they should donate everything to the
    most effective charity.</p>
    <p>Well, <a href="http://youtube.com/watch?v=H8i2tOfzyfk">that was
    then and this is now</a>. Now that we are doing decision theory 102,
    we are not talking about <a
    href="https://forum.effectivealtruism.org/posts/zA6AnNnYBwuokF8kB/is-effective-altruism-growing-an-update-on-the-stock-of#How_many_funds_are_committed_to_effective_altruism_">small
    amounts of money anymore</a>. Consequently, the linear model no
    longer applies and we need to take other factors into account: the
    ability of a charity to <a
    href="http://forum.effectivealtruism.org/tag/room-for-more-funding">absorb
    additional funding</a>, <a
    href="https://economics.stackexchange.com/questions/26892/what-are-general-equilibrium-effects">general
    equilibrium effects</a>, and <a
    href="https://80000hours.org/articles/problem-framework/#how-to-assess-scale">scalability</a>.
    One way to bundle all of these effects into a tractable mathematical
    model is to think about <a
    href="https://forum.effectivealtruism.org/tag/diminishing-returns">diminishing
    returns</a>: investing twice as much input will lead to <em>less
    than</em> twice as much output<sup>[20]</sup>.</p>
    <div class="sidenote">
    <p>[20] </p>
    <p>I’m using a logarithm function, inspired by <a
    href="http://danluu.com/dunning-kruger/#:~:text=think%20they%20know.-,Income%20%26%20Happiness,-It%27s%20become%20common">the
    presumed power-law relationship between income and
    happiness</a>.</p>
    </div>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_145.png" /></p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_146.png" />Same
    as the previous figure, but with a wider range on the x-axis. You
    now notice that the function in the previous figure has been a
    logarithm all along that just <em>looked</em> linear locally.</p>
    <p>When the <strong>x</strong> is small, then we are still in the
    “basically linear” regime and the optimal strategy of not splitting
    the budget remains the same. However, when the <strong>x</strong>
    gets large enough that we experience diminishing returns, then also
    our optimal strategy changes: Now we should split our budget
    proportional to the scaling factor of the
    opportunity<sup>[21]</sup>:</p>
    <div class="sidenote">
    <p>[21] </p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_151.png" /></p>
    </div>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_147.png" /></p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_148.png" />Same
    as…. You know the spiel.</p>
    <p>Now, clearly, if the <strong>a</strong> of some opportunities are
    <a
    href="https://www.effectivealtruism.org/articles/prospecting-for-gold-owen-cotton-barratt/">orders
    of magnitude larger</a> than others, then those other opportunities
    still should receive approximately 0% of our donations (sorry <a
    href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/the-tale-of-gandhi-and-the-devil">sweaters-for-kittens</a>).
    But if there are multiple equally promising opportunities, it is
    o.k. to split your budget.</p>
    <h4 id="concluding-thoughts"><strong>Concluding
    thoughts</strong></h4>
    <p>In this post, I have re-derived some of the community maxims that
    have naturally emerged over the years. In particular, I argued
    that</p>
    <ol class="incremental" type="1">
    <li>the <a
    href="https://link.springer.com/article/10.3758/s13414-015-1018-y">race
    model inequality</a> can be used to argue that it is good to <em>do
    multiple things that have a potentially large positive impact,</em>
    rather than to channel all your energy into just one thing.</li>
    <li>in situations where <a
    href="http://ce.sharif.edu/courses/90-91/1/ce695-1/resources/root/Notes/Lec-8b.pdf">absorbing
    states</a> are relevant (almost always), it can be better to
    optimize for high mean <strong>and</strong> low variance, rather
    than just for a high mean.</li>
    <li>when there are <a
    href="https://forum.effectivealtruism.org/tag/diminishing-returns">diminishing
    returns</a> or when the types of reward from different opportunities
    are not fungible (like different <a
    href="https://www.existential-risk.org/concept.pdf">existential risk
    scenarios</a>), then it is admissible to split donations.</li>
    </ol>
    <p>There is a saying that ” <em>knowing a little economics is worse
    than knowing no economics</em>.”<sup>[22]</sup> <a
    href="https://en.wikipedia.org/wiki/Ludic_fallacy">The implication
    is clear, most models that are easy enough to be taught to a
    beginner are not powerful enough to be useful in the real world</a>.
    I don’t think the situation is <em>that</em> bad for effective
    altruism, knowing a <em>little</em> effective altruism doesn’t make
    your decisions <em>worse</em>. The <a
    href="https://www.youtube.com/watch?v=2HeDdCFZmKc&amp;ab_channel=CentreforEffectiveAltruism">impressive
    track record</a> of effective altruism demonstrates clearly that the
    movement is doing something right.</p>
    <div class="sidenote">
    <p>[22] </p>
    <p>Usually, I have a source for everything, but here I’m pulling a
    blank. Perhaps the statement came to me in a dream. It certainly
    <em>feels</em> true. Edit: Ah, <a
    href="https://www.theatlantic.com/business/archive/2017/01/economism-and-the-minimum-wage/513155/">found
    it</a>!</p>
    </div>
    <p>This is in part because the competition is very weak. Even if you
    <em>naively</em> implement the maxim of maximizing expected utility,
    you will still have more positive impact compared to someone who
    doesn’t think about impact at all. But the reality is complex and
    maximizing utility is not straightforward sometimes, so there are a
    lot of “nontrivial” tricks that can be used to squeeze out even more
    good. A turn <a
    href="https://forum.effectivealtruism.org/posts/LrKFNQxjETPvzXQcv/should-you-switch-away-from-earning-to-give-some">away
    from earning to give</a>, away from career “tracks” and <a
    href="https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists">towards
    career “aptitudes”</a>, and the peaceful co-existence of <a
    href="https://www.givingwhatwecan.org/best-charities-to-donate-to-2021/">diverse
    effective charities</a>, attest that the community is flexible
    enough not to fall into the obvious traps posed by naive utility
    maximization.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_149.png" /></p>
    <div class="debug-grid"></div>
  <script src="../../index.js"></script>
</body>

</html>