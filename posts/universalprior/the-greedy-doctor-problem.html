<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jan Kirchner" />
  <meta name="dcterms.date" content="2021-11-15" />
  <title>The Greedy Doctor Problem</title>
  <link rel="stylesheet" href="../../reset.css" />
  <link rel="stylesheet" href="../../index.css" />
</head>

<body>
  <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title"><a href="https://kirchner-jan.github.io/minimalprior/"
            style="text-decoration: none; color: inherit;">The Greedy
            Doctor Problem</a></h1>
        <span class="subtitle">TL;DR How to reason about people who are
          smarter than you. A few proposals, interspersed with reinforcement
          learning and humorous fiction. Ending on a…</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">2021-11-15</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://kirchner-jan.github.io/minimalprior/">Jan
          Kirchner</a></td>
    </tr>
  </table>
  <nav id="TOC" role="doc-toc">
    <ul class="incremental">
      <li><a href="#what-is-the-greedy-doctor-problem" id="toc-what-is-the-greedy-doctor-problem"><strong>What is the
            Greedy Doctor Problem?</strong></a></li>
      <li><a href="#some-background-on-the-problem" id="toc-some-background-on-the-problem"><strong>Some background
            on the problem</strong></a></li>
      <li><a href="#three-approaches-to-handling-greedy-doctors"
          id="toc-three-approaches-to-handling-greedy-doctors"><strong>Three
            approaches to handling greedy doctors</strong></a></li>
      <li><a href="#truth-values-and-terminal-diseases" id="toc-truth-values-and-terminal-diseases">Truth values and
          terminal diseases</a></li>
    </ul>
  </nav>
  <h3 id="what-is-the-greedy-doctor-problem"><strong>What is the
      Greedy Doctor Problem?</strong></h3>
  <p>I came up with a neat little thought
    experiment<sup>[1]</sup>:</p>
  <div class="sidenote">
    <p>[1] </p>
    <p>This is <em>not</em> a subtweet/sub <em>post</em> (!?) for a<a
        href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/drug-addicts-and-deceptively-aligned">certain
        medical professional</a> that I have recently collaborated with.</p>
  </div>
  <blockquote>
    <p>You are very rich and you want to make sure that you stay
      healthy. But you don’t have any medical expertise and, therefore,
      you want to hire a medical professional to help you monitor your
      health and diagnose diseases. The medical professional is greedy,
      i.e. they want to charge you as much money as possible, and they do
      not (per se) care about your health. They only care about your
      health as far as they can get money from you. How can you design a
      payment scheme for the medical professional so that you actually get
      the ideal treatment?</p>
  </blockquote>
  <p>Over the last few weeks, I’ve been walking around and <a
      href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/applied-mathematical-logic-for-the">bugging
      people</a> with this question to see what they come up with. Here I
    want to share some of the things I learned in the process with you,
    as well as some potential answers. I don’t think the question (as
    presented) is completely well-formed, so the first step to answering
    it is clarifying the setup and <a
      href="https://www.lesswrong.com/posts/5Nz4PJgvLCpJd6YTA/looking-deeper-at-deconfusion">deconfusing</a>
    the terms. Also, as is typical with thought experiments, I do not
    have a definitive “solution” and invite you (right now!) to try and
    come up with something yourself<sup>[2]</sup>.</p>
  <div class="sidenote">
    <p>[2] </p>
    <p>If you come up with something clever, feel free to shoot me an
      email or leave a comment.</p>
  </div>
  <h3 id="some-background-on-the-problem"><strong>Some background on
      the problem</strong></h3>
  <p>The subtext for the thought experiment is: How should you act
    when interacting with someone <a
      href="https://www.lesswrong.com/posts/kXSETKZ3X9oidMozA/the-level-above-mine">smarter
      than yourself</a>? What can you say or do, when your interlocutor
    has thought of everything you might say and more? Should you
    <em>trust</em> someone’s advice, when you can’t pinpoint their
    motivation? As a Ph.D. student, I run into this problem around three
    to five times a week, when interacting with colleagues or my
    advisor<sup>[3]</sup>.
  </p>
  <div class="sidenote">
    <p>[3] </p>
    <p>This is of course supposed to be funny, but there is the real
      problem of inferring the motivation of a supervisor or collaborator
      when they say “Let’s work a bit more on this before graduating.”
      Incentives here are often misaligned, where an experienced grad
      student is a comparatively cheap source of labor up until
      graduation.</p>
  </div>
  <p>After bugging a few people I learned that (<a href="https://www.youtube.com/watch?v=nJPERZDfyWc">of course</a>)
    I’m not the first person to think about this question. In economics
    and political science, the situation is known as the <a
      href="https://www.investopedia.com/terms/p/principal-agent-problem.asp">principal-agent
      problem</a> and is defined as ” <em>a conflict in priorities between
      a person or group and the representative authorized to act on their
      behalf. An agent may act in a way that is contrary to the best
      interests of the principal.</em> ” This problem arises f.e. in the
    context of conflicts between<a href="https://www.investopedia.com/updates/enron-scandal-summary/">corporate
      management and shareholders</a>,<a href="https://www.jstor.org/stable/724478">clients and their
      lawyers</a>, or<a href="https://www.jstor.org/stable/40751249">elected officials and
      their voters</a>. Well-trodden territory.</p>
  <p>With decades of literature from different academic fields, <a href="https://equilibriabook.com/toc/">can we really
      expect</a> to
    contribute anything original? I hope so, in particular since all the
    previous research on the topic is<a
      href="https://www.lesswrong.com/posts/Z5ZBPEgufmDsm7LAv/what-can-the-principal-agent-literature-tell-us-about-ai">constrained
      to “realistic” solutions and bakes in a lot of assumptions about how
      humans operate</a>. That’s not the spirit of this thought
    experiment. Do you want to think about whether sending the doctor in
    a rocket to Mars might help? Please do<sup>[4]</sup>. Don’t let
    yourself be constrained by practicalities<sup>[5]</sup>.</p>
  <div class="sidenote">
    <p>[4] </p>
    <p>Although there is of course also something to be said about the<a
        href="https://www.jstor.org/stable/42970833">limits of thought
        experiments</a>.</p>
  </div>
  <div class="sidenote">
    <p>[5] </p>
    <p>Additionally, I reject the framing that things have to be novel
      to be interesting. Just because the thought is not new to everyone
      it might still be new to me and you (dear reader) and<a
        href="https://www.lesswrong.com/posts/KfMNFB3G7XNviHBPN/joy-in-discovery">it
        can still be satisfying to rediscover things</a>.</p>
  </div>
  <p>In this spirit, let us think about the problem from the
    perspective of <a href="https://faculty.ai/blog/what-is-ai-safety/">interactions
      between abstract intelligent agents</a>. Here,<a
      href="https://arbital.greaterwrong.com/p/Vinge_principle?l=1c0">Vinge’s
      principle</a> is relevant: <em>in domains complicated enough that
      perfect play is not possible, less intelligent agents will not be
      able to predict the</em> exact <em>moves made by more intelligent
      agents.</em> The reasoning is simple; if you were able to predict
    the actions of the more intelligent agent exactly, you could execute
    the actions yourself and effectively act at least as intelligent as
    the “more intelligent” agent - a contradiction<sup>[6]</sup>. In the
    greedy doctor thought experiment, I assume the doctor to be
    uniformly more knowledgable than me, therefore Vinge’s principle
    applies.</p>
  <div class="sidenote">
    <p>[6] </p>
    <p>When we define intelligence in terms of behavior. Which is
      reasonable, I think - how else is it going to manifest?</p>
  </div>
  <p>While this impossibility result is prima facie discouraging, it
    reveals a useful fact about the type of uncertainty involved. Both
    you and the doctor have access to the same facts<sup>[7]</sup> and
    have the same amount of <a
      href="https://en.wikipedia.org/wiki/Uncertainty_quantification#:~:text=game%20of%20chance.-,Epistemic%20uncertainty,-Epistemic%20uncertainty%20is">epistemic
      uncertainty</a>. The difference in uncertainty between you and the
    doctor is instead due to differences in computational capacity; it
    is <a href="https://golem.ph.utexas.edu/category/2016/09/logical_uncertainty_and_logica.html">logical
      uncertainty</a>. Logical uncertainty behaves <a
      href="https://intelligence.org/files/QuestionsLogicalUncertainty.pdf">fairly
      differently from epistemic uncertainty</a>; in particular, <a
      href="https://intelligence.org/files/LogicalInduction.pdf">different
      mathematical tools are required to operate on
      it</a><sup>[8]</sup>.</p>
  <div class="sidenote">
    <p>[7] </p>
    <p>You both have access to Google.</p>
  </div>
  <div class="sidenote">
    <p>[8] </p>
    <p>This will become relevant later. ~~ Foreshadowing much ~~</p>
  </div>
  <p>But having said all that, I have not encountered any satisfying
    proposals for how to approach the problem, nor convincing arguments
    for why these approaches fail. So let’s think about it
    ourselves.</p>
  <h3 id="three-approaches-to-handling-greedy-doctors"><strong>Three
      approaches to handling greedy doctors</strong></h3>
  <p>Here is how I think about the situation:</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_12.png" />There
    is a ground truth “observation” about whether you are actually sick
    or not. Only the doctor has access to that observation and makes a
    diagnosis that might or might not be based on the diagnosis. You,
    the patient, receive the diagnosis and decide whether or not to pay
    the doctor.</p>
  <p>This is a (slightly pathological) <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov
      Decision Process</a><sup>[9]</sup>. The observations come from a set
    of states <strong>S</strong> , which I model as a fair coin
    flip<sup>[10]</sup>. “Tails” is ” <strong>t</strong> reatment” and
    “Heads” is ” <strong>h</strong> ealthy”. Similarly, the diagnosis of
    the doctor comes from a set of actions <strong>A</strong> , where
    the doctor can either declare that the patient needs ”
    <strong>T</strong> reatment” or is ” <strong>H</strong> ealthy”. The
    payment from the patient to the doctor is the reward, which is a
    function <strong>Φ</strong> that only depends on the diagnosis of
    the doctor, not on the actual observation. Finally, the strategy
    according to which the doctor diagnoses the patient is a policy
    <strong>π</strong> , which assigns each possible diagnosis a
    probability given the observation.
  </p>
  <div class="sidenote">
    <p>[9] </p>
    <p>I knew from the start that Substack is not the ideal platform for
      writing math-heavy posts. While this makes my life harder, there is
      also the possibility that I’m cornering a market here that nobody
      else can be bothered to corner. So let’s don-quixote it!</p>
  </div>
  <div class="sidenote">
    <p>[10] </p>
    <p>I’m pretty sure biased coins work analogously.</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_13.png" />Feel
    free to ignore the squiggles, it’s just a fancy way of saying what I
    just said in the preceding paragraph.</p>
  <p>What does this set-up buy us?</p>
  <h4 id="scenario-one-just-pay-the-doctor-dammit."><strong>Scenario
      one: just pay the doctor, dammit.</strong></h4>
  <p>This first approach appears silly after setting up all the
    mathematical apparatus, but I include it since I got this suggestion
    from one or two people: Why don’t we just pay the doctor when they
    diagnose something?</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_14.png" /></p>
  <p>In their defense, this is a very reasonable approach when we
    model the doctor as at least partially human. However, when we model
    the doctor as <a
      href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/drug-addicts-and-deceptively-aligned">truly
      greedy</a><sup>[11]</sup>, we observe a very <a
      href="https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity">familiar
      failure mode</a>. If you pay the doctor for every time they diagnose
    a disease, <a href="https://en.wikipedia.org/wiki/Point_estimation">they will
      diagnose you</a> with <em>everything</em> and take the money - and
    the treatment will not actually be good for you. I think this would
    a bit like the following scenario<sup>[12]</sup>:</p>
  <div class="sidenote">
    <p>[11] </p>
    <p>i.e. an agent trying to maximize reward.</p>
  </div>
  <div class="sidenote">
    <p>[12] </p>
    <p>I think philosophical arguments in the form of a fiction novel <a
        href="https://en.wikipedia.org/wiki/Atlas_Shrugged">tend to be
        terrible</a>. Mathematical arguments <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">augmented
        with fictional interludes</a>, however…</p>
  </div>
  <blockquote>
    <p><a
        href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/soldiers-scouts-and-albatrosses">Albert</a>:
      Yes Dr. Jones, what is it?<br />
      Dr. Jones: Ahhh, Albert! Good that I finally reach you. Did you not
      get my other calls?<br />
      Albert: The previous 37 calls that went to voice mail where you get
      increasingly exasperated and say that I have to come see you?<br />
      Dr. Jones: …<br />
      Albert: …<br />
      Dr. Jones: …<br />
      Albert: I must have missed those.<br />
      Dr. Jones: Ah, I see. My apologies for the insistence, but I assure
      you, I only have your best at heart. I had another look at the blood
      work.<br />
      Albert: …<br />
      Dr. Jones: …<br />
      Albert: …<br />
      Dr. Jones: …<br />
      Albert: … <em>sigh</em> What is it this ti-<br />
      Dr. Jones: WATER ALLERGY!<br />
      Albert: Don’t be ri-<br />
      Dr. Jones: Albert, dear boy, listen to me. Please listen to me, this
      is a matter of (your!) life and death. Stay away from water in any
      way, shape or form. No swimming, bathing, showering or taking a
      stroll in a light drizzle. And come to my office as soon as
      possible. We have to commence treatment immediately. Immediately, do
      you understand? Your insurance is still…?<br />
      Albert: …<br />
      Dr. Jones: …<br />
      Albert …<br />
      Dr. Jones: …<br />
      Albert: Yes, it is sti-<br />
      Dr. Jones: Great! Great news. Okay, no more time to quiddle. I’ve
      sent you a taxi to pick you up in five. Wait outside.<br />
      Albert: But it’s raining?<br />
      Dr. Jones: <em>hung up</em></p>
  </blockquote>
  <p>If you pay them whenever you are diagnosed as healthy, they will
    diagnose exactly that. A flat rate is independent of whether they
    diagnose anything, and they will behave randomly. When you impose an
    “objective metric” like heart-rate variability, they will <a
      href="https://en.wikipedia.org/wiki/Goodhart%27s_law">goodhart</a>
    it.</p>
  <p>So that you don’t just have to trust me that something like this
    is bound to happen in this set-up, here is what happens when I train
    a reinforcement agent with Q-Learning<sup>[13]</sup> with the
    proposed reward function:</p>
  <div class="sidenote">
    <p>[13] </p>
    <p>I’m using vanilla Q-Learning, because “<a href="https://andyljones.com/posts/rl-debugging.html">why make my
        life difficult</a>?”</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_15.png" />
    <strong>A greedy doctor incentivized to diagnose “treatment” will
      diagnose treatment a lot.</strong> <strong>a</strong> Reward of the
    agent per epoch averaged over 300 runs. Dashed line indicates
    maximal reward possible (<a
      href="https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/">epsilon-greedy</a>
    with Ɛ = 5%). <strong>b</strong> Fraction of deciding ”
    <strong>t</strong> reatment” per epoch, averaged over 300 runs.
    Dashed line indicates chance level. <strong>c</strong> Fraction of
    correct decisions per epoch averaged over 300 runs. Dashed line
    indicates chance level.
  </p>
  <p>This is a classic case of <a href="https://www.lesswrong.com/tag/outer-alignment">outer
      alignment</a> failure: The thing we wrote down does not actually
    capture the thing we care about. Try again.</p>
  <h4 id="scenario-two-do-the-obvious-thing."><strong>Scenario two: Do
      the obvious thing.</strong></h4>
  <p>The second proposed solution is <em>very</em> commonsensical:
    Just <a href="https://www.webmd.com/a-to-z-guides/features/how-to-ask-for-second-opinion">ask
      for a second opinion</a> and <em>only</em> pay the doctors when they
    come to the same conclusion. While this <em>sounds</em> clever, it
    falls into the same trap as before. When both doctors are
    <em>greedy</em> , they will coordinate and both <em>always</em> say
    that you are either healthy or that you need treatment.
  </p>
  <p>However, with a little twist we can get closer to a solution:
    Reward one doctor <em>only</em> if both doctors say you’re healthy.
    Reward the other doctor <em>only</em> if both doctors say you
    require treatment.</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_16.png" />As
    before, the ground truth is determined by observation. But this
    time, it is shared between two doctors, who each get to give an
    independent diagnosis. Reward is only handed out when both doctors
    agree. Doctor A only gets paid when both doctors diagnose “
    <strong>h</strong> ealthy”. Doctor B only gets paid when both
    doctors diagnose “ <strong>t</strong> reatment“.
  </p>
  <p>This payment rules out scenarios where both doctors <em>only</em>
    diagnose whatever they get paid for. It also disincentivizes random
    behavior, since then each doctor will only get paid when both
    doctors coincidentally say whatever one doctor gets paid for (1/4 of
    the cases). The doctors can get twice the reward by cooperating and
    coordinating their diagnosis with the other doctor. The shared
    observation (whether you are truly <strong>h</strong> ealthy or
    require <strong>t</strong> reatment) can serve as a useful <a
      href="https://en.wikipedia.org/wiki/Focal_point_%28game_theory%29">Schelling
      point</a> for coordination between the doctors.</p>
  <p>Getting two reinforcement agents to (reliably) cooperate is hard
    enough to <a href="https://www.science.org/doi/full/10.1126/science.aau6249">get
      you a paper in Science</a>. When I naively implement two Q-Learning
    agents with the depicted payment, they are uncooperative: either
    exclusively diagnoses <strong>H</strong> or <strong>T</strong> ,
    forsaking the dominant strategy of cooperation. This mirrors a
    famous problem in game theory called the “<a
      href="https://en.wikipedia.org/wiki/Battle_of_the_sexes_%28game_theory%29">Battle
      of the sexes</a>”.</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_17.png" />The
    reward is decreasing, which is <a href="https://link.springer.com/article/10.1007%2FBF00992698">not
      supposed to happen</a>. But of course, the usual convergence proof
    does not allow for a changing environment/reward function.</p>
  <p>This is already getting way too complicated. <a
      href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/on-scaling-academia">I’m
      not trying to publish in Science, I’m just trying to solve a
      problem</a><sup>[14]</sup>. Since I expect that a<a
      href="https://www.science.org/doi/full/10.1126/science.aau6249">more
      sophisticated reinforcement learning approach</a> will get the
    agents to cooperate, I’ll make my life easier<sup>[15]</sup> by just
    forcing the agents to cooperate<sup>[16]</sup>.</p>
  <div class="sidenote">
    <p>[14] </p>
    <p>Although, if you are one of the many Science editors reading this
      Substack, hmu.</p>
  </div>
  <div class="sidenote">
    <p>[15] </p>
    <p>I force them by normalizing the columns of the Q-matrix at every
      time step to 1. This forces the agent to pick <strong>H</strong> and
      <strong>T</strong> equally often.
    </p>
  </div>
  <div class="sidenote">
    <p>[16] </p>
    <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_2.gif" /></p>
  </div>
  <p>Once we force the doctors to cooperate, we find that the reward
    goes up, the fractions of ” <strong>t</strong> reatment” and ”
    <strong>h</strong> ealthy” diagnoses are nice and balanced and the
    correspondence with ground truth… <em>wait what</em>?
  </p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_18.png" /></p>
  <p>Ah, of course. Just because we picked nice, suggestive labels for
    the observation ( <strong>T</strong> and <strong>H</strong> ) and
    the diagnosis ( <strong>t</strong> and <strong>h</strong> ), the
    agent doesn’t care about that at all. In half of the cases, the
    doctors will cooperate by always diagnosing the opposite of what
    they observe. They still get paid, but the performance drops
    dramatically below the chance level. I call these doctors “trolling
    doctors”, even though there is <a href="https://www.goodreads.com/en/book/show/44154569-the-ai-does-not-hate-you">no
      malice required</a> - <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor">just
      negligence</a> on part of the programmer<sup>[17]</sup>.</p>
  <div class="sidenote">
    <p>[17] </p>
    <p>When the number of possible diagnoses increases, this gets worse
      as every permutation is possible. However, every permutation is
      guaranteed to give below chance performance and might be
      detectable.</p>
  </div>
  <p>Well, perhaps it is not so bad. We might be able to fix it; <a
      href="https://www.lesswrong.com/posts/qNZM3EGoE5ZeMdCRt/reversed-stupidity-is-not-intelligence">somebody
      who always lies is basically as useful as someone who always tells
      the truth</a>. We just have to do the exact opposite of what they
    recommend. And as long as there is <em>some</em> real-world
    consequence of the diagnosis of the doctor, we might be able to
    identify below-chance performance, by comparison with an agent that
    predicts at chance level<sup>[18]</sup>.</p>
  <div class="sidenote">
    <p>[18] </p>
    <p>Like f.e. the agent from scenario one.</p>
  </div>
  <p>But the situation is worse than that. As long as the action
    policies of the two agents match<sup>[19]</sup>,</p>
  <div class="sidenote">
    <p>[19] </p>
    <p>This of course automatically satisfies the cooperation
      constraint.</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_19.png" /></p>
  <p>, they will get maximum reward. The doctors could play <a
      href="https://en.wikipedia.org/wiki/Tit_for_tat">tit-for-tat</a>,
    where they alternate between both diagnoses, ” <strong>h</strong>
    ealthy” and ” <strong>t</strong> reatment”, independent of what the
    coin flip says. I imagine the following scenario:</p>
  <blockquote>
    <p>A seedy bar with perdition thick in the air. A woman in a
      trenchcoat sits in a dimly lit corner, smoking a cigar. A second
      woman in a trenchcoat, collar up, enters the bar, casting her eyes
      across tables and seats, spotting the first woman, and moving
      towards her with fortitude.<br />
      Dr. Jones: Dr. Firenze, glad you could make it.<br />
      Dr. Firenze: Of course.<br />
      <em>Dr. Firenze nervously licks her lips.</em><br />
      Dr F: I heard Mr. A was not amused regarding the recent…
      discrepancies…<br />
      Dr. J: Discrepancies! The nerve of you!<br />
      Dr. F: The man is clearly in perfect health.<br />
      Dr. J: Foreign Accent Syndrome is no laughing matter! He is speaking
      in tongues.<br />
      Dr. F: The man is from <em>Ireland</em>.<br />
      Dr. J: …<br />
      Dr. F: …<br />
      Dr. J: How about a truce?<br />
      Dr. F: A truce!<br />
      Dr. J: A truce. We both don’t want to work. We both want to get
      paid. How about - You scratch my back and I’ll scratch yours. One
      for me, one for you. You catch my drift…
    </p>
  </blockquote>
  <p>Clearly, we’d want to forbid any communication between the
    doctors. However, it appears here we run into a <a
      href="https://www.lesswrong.com/posts/5wMcKNAwB6X4mp9og/that-alien-message">very
      tricky problem</a>: Even when we manage to stop any message
    transmitted between the doctors directly, they will still be able to
    infer the diagnosis of the other doctor from whether they got paid
    or not<sup>[20]</sup>. If the doctors are sufficiently intelligent
    (and willing to forsake payment for a while), they will be able to
    use this channel to transfer arbitrary messages in binary between
    each other<sup>[21]</sup>. Or, if the doctors are sufficiently
    similar, they might even use <a
      href="https://medium.com/how-to-build-an-asi/how-to-do-functional-decision-theory-b9035ca05812#:~:text=Psychological%20Twin%20Prisoner%E2%80%99s%20Dilemma,-Taken%20from%20Yudkowsky">fancy
      decision theory</a> to coordinate without having to communicate at
    all.</p>
  <div class="sidenote">
    <p>[20] </p>
    <p>Since they get paid iff the other doctor gets paid also.</p>
  </div>
  <div class="sidenote">
    <p>[21] </p>
    <p>This communication might be hampered by lumping together the
      payoff to the end of the epoch or by adding noise to the payoff. The
      fundamental problem remains, however.</p>
  </div>
  <p>Just forcing the doctors to cooperate leaves too many doors open.
    We need something more… <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">adversarial</a>.</p>
  <h4 id="scenario-three-do-the-other-obvious-thing.">Scenario three:
    Do the <em>other</em> obvious thing.</h4>
  <p>The other most common-sensical answer that I heard a lot can be
    summarized in two words: <a href="https://www.youtube.com/watch?v=wqnHtGgVAUE">lawyer up</a>.
    Instead of trusting in the goodwill of the doctor, or letting them
    come up with a cozy cooperative deal, make them work for their
    money. The idea is simple: get a detective to investigate what the
    doctor is doing. If it looks shady, don’t pay the doctor.</p>
  <p>However, the word “shady” hides a lot of complexity here. Always
    diagnosing ” <strong>t</strong> reatment” is shady. Alternating
    between ” <strong>t</strong> reatment” and ” <strong>h</strong>
    ealthy” is shady. But there are a lot more complex binary strings
    that look random, but don’t correspond to the actual “H” and “T”
    strings. How can we tell if the doctor is being honest?</p>
  <p>Consider the following set-up:</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_20.png" />This
    time, only one doctor makes the observation and diagnosis. However,
    this time a detective gets to see the previous diagnoses and can try
    to infer a pattern. When the detective successfully predicts a
    pattern, the detective gets paid and the doctor doesn’t get paid.
    When the doctor diagnoses differently from what the detective
    predicts, the doctor gets paid and the detective doesn’t.</p>
  <p>My doctor is again a Q-Learning agent and my detective is an
    autoregressive model that tries to predict the next action of the
    doctor given the last ten actions<sup>[22]</sup>. In this set-up,
    the agent learns pretty nicely:</p>
  <div class="sidenote">
    <p>[22] </p>
    <p>There are some very easy models that are very good at doing this
      kind of prediction. Check out this game <a
        href="https://people.ischool.berkeley.edu/~nick/aaronson-oracle/">here</a>.</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_21.png" /></p>
  <p>The highest reward that the doctor can now get is with random
    performance (Reward = 1/2) since every non-random pattern will be
    picked up by the detective. Since a Q-Learning agent doesn’t have a
    random number generator built-in, the only solution is to harness
    the “randomness” of the observation. We also find that half of the
    doctors report the opposite of the observation (“trolling doctors”),
    but that’s just something we’ll have to live with.</p>
  <p>What do we do in the case of a more sophisticated doctor that
    might have a random number generator available? Running that
    generator would result in the same reward as forwarding the
    randomness from the observation. However, <a
      href="https://www.americanscientist.org/article/the-quest-for-randomness">true
      randomness is really hard to get by</a> and computers usually use
    routines that produce numbers that <a href="https://en.wikipedia.org/wiki/Pseudorandomness">look random,
      but aren’t really</a>. If the detective was an exact clone of the
    doctor<sup>[23]</sup>, they would have access to the exact same
    random number generators as the doctor. Whatever routine the doctor
    uses to determine a <a href="https://en.wikipedia.org/wiki/Random_seed">seed</a> for the
    generator, the detective would come up with the same routine. The
    only source of randomness that the doctor has access to, but the
    detective doesn’t, is (by construction) the observation.</p>
  <div class="sidenote">
    <p>[23] </p>
    <p>Kind of like a <a href="https://www.arthurconandoyle.com/professormoriarty.html">Holmes
        vs. Moriarty</a> situation.</p>
  </div>
  <h3 id="truth-values-and-terminal-diseases">Truth values and
    terminal diseases</h3>
  <p>This leaves us with one<sup>[24]</sup> central conundrum: When is
    it appropriate to model reality as a flip of a coin? I’m not sure. I
    guess at this point the doctor analogy has served its usefulness and
    it makes sense to talk about advanced AI instead. The central
    question then translates into: How can we incentivize an advanced AI
    (that might be smarter than us) to tell us truthfully about its
    inferences - even though those inferences completely fly over our
    heads and we have no way of verifying the truthfulness.</p>
  <div class="sidenote">
    <p>[24] </p>
    <p>At least one? Have kind of lost count.</p>
  </div>
  <p>As I’ve mentioned in the introduction, this is a scenario in
    which <a href="https://www.lesswrong.com/tag/logical-uncertainty">logical
      uncertainty</a> applies: We are uncertain about the implications of
    certain beliefs. An example of this might be asking the AI whether a
    certain strategy will have a net positive or negative impact on a
    certain measure we care about. Even if we have access to the same
    information as the AI, we might still be substantially more
    uncertain about the impact. This additional uncertainty stems from
    our lack of logical omniscience. We cannot reason through the
    implications of the available information completely. An AI might do
    so a lot more successfully, and thus be less uncertain about the
    impact.</p>
  <p>The proposed solution, a doctor-detective tandem, shares certain
    features of the <a href="https://intelligence.org/files/LogicalInduction.pdf">logical
      induction</a> paradigm from <a href="https://intelligence.org/files/LogicalInduction.pdf">Garrabrant
      et al</a>. Like Garrabrant’s traders that attempt to predict the
    market price of certain logical propositions, our detective attempts
    to predict the diagnosis of the doctor. Like the stable market fixed
    point, at which no trader can extract unlimited resources from the
    market, the fixed point of our doctor-detective tandem is achieved
    when the doctor’s diagnoses cannot be predicted by the detective.
    Perhaps, with some more wiggling, we can turn the tandem into a full
    logical inductor, along with <a href="https://www.youtube.com/watch?v=gDqkCxYYDGk">all the nice
      properties that follow</a>. I’m sure there are many things that are
    missing to make the parallels complete<sup>[25]</sup>, but I already
    had too much fun thinking about this. So I’m <a
      href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/how-to-build-a-mind-neuroscience">putting
      it out there</a> to hear if anyone has more thoughts about this.</p>
  <div class="sidenote">
    <p>[25] </p>
    <p>And I’m even more sure that I’ve made a couple of invalid
      inferences throughout the post that might invalidate certain
      portions.</p>
  </div>
  <p>If you want to hear more of <em>my</em> thoughts, consider
    signing up for the newsletter!</p>
  <div class="debug-grid"></div>
  <script src="../../index.js"></script>
</body>

</html>