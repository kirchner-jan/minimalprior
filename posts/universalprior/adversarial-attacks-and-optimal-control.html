<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="" >

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="Jan Kirchner" />
          <title>minimalprior</title>
    <link rel="stylesheet" href="../../reset.css" />
    <link rel="stylesheet" href="../../index.css" />
      </head>

<body>
    <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title">minimalprior</h1>
        <span class="subtitle">a spinoff</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">YYYY-MM-DD</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://universalprior.substack.com/">Jan
Kirchner</a></td>
    </tr>
  </table>
      <nav id="TOC" role="doc-toc">
        <ul class="incremental">
        <li><a href="#date-2022-05-22" id="toc-date-2022-05-22">date:
        2022-05-22</a>
        <ul class="incremental">
        <li><a href="#tldr"
        id="toc-tldr"><strong>TL;DR:</strong></a></li>
        </ul></li>
        <li><a href="#real-talk-and-real-estate"
        id="toc-real-talk-and-real-estate"><strong>Real talk and real
        estate</strong></a></li>
        <li><a href="#a-random-environment"
        id="toc-a-random-environment"><strong>A random
        environment</strong></a></li>
        <li><a href="#average-surprise-and-optimal-control"
        id="toc-average-surprise-and-optimal-control"><strong>Average
        surprise and optimal control</strong></a></li>
        <li><a href="#an-adversarial-environment"
        id="toc-an-adversarial-environment"><strong>An adversarial
        environment</strong></a></li>
        </ul>
  </nav>
    <h2 id="date-2022-05-22">date: 2022-05-22</h2>
    <p><em>Meta: After a fun little motivating section, this post goes
    pretty deep into the mathematical weeds - you might prefer to read
    it on<a
    href="https://www.lesswrong.com/posts/KtCJNw93KHg7MSSvw/adversarial-attacks-and-optimal-control">LessWrong</a>
    with proper typesetting instead of my creative use of
    Unicode.</em></p>
    <h3 id="tldr"><strong>TL;DR:</strong></h3>
    <ol class="incremental" type="1">
    <li>When we invest the appropriate effort, the probability of random
    catastrophic events tends to decrease exponentially with a rate
    given by the<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-1-56106015">1</a>
    rate function.</li>
    <li>One way of estimating the rate function is to solve an optimal
    control problem, where we have to determine the “least surprising”
    way that the catastrophic event comes about. The rate function then
    equals the catastrophic event’s surprise (in <a
    href="https://en.wikipedia.org/wiki/Nat_(unit)">nats</a>).</li>
    <li>Thus, intuitively, as we invest the effort to decrease the
    probability of random catastrophic events, the “difficulty” of
    performing an adversarial attack only increases linearly.</li>
    </ol>
    <h1 id="real-talk-and-real-estate"><strong>Real talk and real
    estate</strong></h1>
    <p>Zillow is an American tech real-estate marketplace company that
    recently had (what the experts call) a small <a
    href="https://en.wikipedia.org/wiki/SNAFU">snafu</a>. They decided
    they were done <em>just</em> being a marketplace and started <a
    href="https://www.businessinsider.com/ibuyer-defunct-why-did-zillow-stop-buying-houses-2021-11">buying
    up homes, completing light renovations, and then selling them with a
    profit</a>. The whole thing went poorly; they bought houses too
    expensive and had to sell at a loss, <a
    href="https://www.nytimes.com/2021/11/02/business/zillow-q3-earnings-home-flipping-ibuying.html">costing
    the company $420 million and leading to large lay-offs</a>.</p>
    <p>This story is not very interesting<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-2-56106015">2</a>
    for anyone who’s not directly involved. The reason I remember the
    whole affair is a <a
    href="https://twitter.com/ESYudkowsky/status/1456279486840721414?s=20&amp;t=kqTdW87iwMg3f9zhqYKQ_w">Twitter
    thread</a> that also caught the attention of Eliezer:</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab62d7fb-cdae-489d-8c6c-1f902796dac6_1160x794.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fab62d7fb-cdae-489d-8c6c-1f902796dac6_1160x794.png" /></a></p>
    <p>The thread lays out how Zillow relied too much on their <a
    href="https://en.wikipedia.org/wiki/Zillow#Zestimate">automatic
    estimates</a>, which (<a
    href="https://universalprior.substack.com/p/slightly-advanced-decision-theory?s=w#:~:text=is%20coming%20from.-,Diverse%20detours.,-On%20the%20topic">ex-ante</a>)
    looked good on average, but (ex-post) manifested in the worst
    possible way<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-3-56106015">3</a>.
    Or, in the words of the OP:</p>
    <blockquote>
    <p>“[They mistook] an adversarial environment for a random one.”</p>
    </blockquote>
    <p>I have approximately zero knowledge about Zillow or the real
    estate business, so I can’t comment on whether this actually is what
    went wrong. But the distinction between a <strong>random</strong>
    and an <strong>adversarial</strong> environment applies beyond the
    Zillow example and is relevant for research in AI Safety: from <a
    href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">adversarial
    examples</a>, via <a
    href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood">adversarial
    training</a>, all the way to conceivable <a
    href="https://en.wikipedia.org/wiki/AI_box">existential risk
    scenarios</a>. It is the distinction between a world where it’s fine
    to go outside because <a
    href="https://www.cdc.gov/disasters/lightning/victimdata.html">getting
    struck by lightning is sufficiently unlikely</a> and a world where
    <a
    href="https://unsongbook.com/chapter-3-on-a-cloud-i-saw-a-child/#:~:text=ACROSS%20ALL%20SEPHIROT.-,I%20THINK%20SOMEBODY%20BOILED%20A%20GOAT%20IN%20ITS%20MOTHER%E2%80%99S%20MILK.%20IT%20IS%20ALWAYS%20THAT.%20I%20KEEP%20TELLING%20PEOPLE%20NOT%20TO%20DO%20IT%2C%20BUT%20NOBODY%20LISTENS,-.%E2%80%9D">somebody
    is trying to smite you</a>.</p>
    <p>This post works towards achieving a mathematical understanding of
    what distinguishes <a
    href="https://people.wou.edu/~shawd/mediocristan--extremistan.html">these
    two worlds</a>. Perhaps other people are not as confused about this
    point as I was, but hopefully, the arguments are still useful and/or
    interesting to some. I’ll introduce a bit of <a
    href="https://en.wikipedia.org/wiki/Extreme_value_theory">extreme
    value theory.</a> Then I’ll demonstrate a neat connection to <a
    href="https://en.wikipedia.org/wiki/Control_theory">control
    theory</a> via <a
    href="https://en.wikipedia.org/wiki/Large_deviations_theory">large
    deviation theory</a>, which shows that an exponentially decreasing
    risk of failure translates into a linearly increasing difficulty of
    adversarial attacks.</p>
    <h1 id="a-random-environment"><strong>A random
    environment</strong></h1>
    <p>I’ll go over the basics quickly: When you (i.e. Zillow) think of
    your environment as random, each house you buy is essentially a coin
    flip with a biased coin. If you make sure that 1. the expected value
    of a deal is positive and 1. that all the deals are (<a
    href="https://en.wikipedia.org/wiki/Martingale_central_limit_theorem">mostly)</a>
    independent of each other,</p>
    <p>then you’re guaranteed to accrue profits <em>eventually</em>.</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfa49339-63ec-44b4-853e-93f7582f5522_3749x998.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfa49339-63ec-44b4-853e-93f7582f5522_3749x998.png" /></a>An
    illustration of how a sequence of house buys will end up net
    positive if repeated sufficiently long.</p>
    <p>This guarantee is the <a
    href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of
    large numbers</a> from probability theory 101 and probably not too
    revolutionary for you, dear reader. But what is the probability of
    things going very badly? What is the probability of an <a
    href="https://en.wikipedia.org/wiki/Extreme_value_theory">extreme
    event</a> or a <a
    href="https://en.wikipedia.org/wiki/Large_deviations_theory">large
    deviation</a>? It should decrease (given the law of large numbers),
    but how <em>fast</em> does it decrease?</p>
    <p>Let’s continue the example of buying and selling houses and say
    that there is a probability p that a given deal ends up profitable.
    The worst-case probability that <em>all</em> your deals go poorly is
    (1−p)ᴺ, which goes to 0 exponentially fast as we increase the number
    of deals N.</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54ebb57e-559f-4d64-8422-87acddd63b81_1162x1311.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F54ebb57e-559f-4d64-8422-87acddd63b81_1162x1311.png" /></a></p>
    <p>It turns out that this toy example <a
    href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_%28large_deviations%29">generalizes
    a lot</a>, and the probability that the sum of independent random
    variables Xᵢ is less than some fraction ξ times N
    <del>always</del><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-4-56106015">4</a>
    goes down exponentially fast in N:</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8257d7fb-ef75-4d59-8a55-cb50bd613d3e_654x196.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8257d7fb-ef75-4d59-8a55-cb50bd613d3e_654x196.png" /></a></p>
    <p>The function J(ξ) is called the “<a
    href="https://en.wikipedia.org/wiki/Rate_function">rate
    function,</a>” and it determines the speed with which extreme events
    get unlikely.</p>
    <p>How to compute the rate function<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-5-56106015">5</a>?
    That is what we’ll explore in the next section. There is a “<a
    href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_(large_deviations)">standard</a>”
    way of calculating J(ξ), which is a bit involved and opaque<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-6-56106015">6</a>
    for my taste; I prefer a slightly non-standard derivation<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-7-56106015">7</a>
    of arriving at the rate function, which gives deeper insight into
    how adversarial attacks and extreme events relate.</p>
    <h1 id="average-surprise-and-optimal-control"><strong>Average
    surprise and optimal control</strong></h1>
    <p>As I like to say: <a
    href="https://youtu.be/1bSPNboKCzM?t=966">there is, of course,
    another way of looking at this.</a> We can think of the sequence of
    house deals as a sequence of random events that fluctuate around the
    probability of failure 1−p. </p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F94516838-1b07-4463-beb6-fff0ae886104_983x915.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F94516838-1b07-4463-beb6-fff0ae886104_983x915.png" /></a></p>
    <p>For a truly random sequence of events, we can compute the
    <strong><a
    href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">average
    surprise</a></strong> by averaging the information content of all
    events:</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F84166a25-2ba2-4b7b-92e4-05f0d987505f_592x144.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F84166a25-2ba2-4b7b-92e4-05f0d987505f_592x144.png" /></a></p>
    <p>where the <strong><a
    href="https://en.wikipedia.org/wiki/Information_content">information
    content</a></strong> of an individual event is given as</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1bf4c50b-3ece-41e8-8239-408dccfb7702_388x100.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1bf4c50b-3ece-41e8-8239-408dccfb7702_388x100.png" /></a></p>
    <p>and the probability measure Pₚ(x) is a Bernoulli measure, Pₚ(x) =
    p when x is 1 and 1-p when x is 0. In the picture, the probability
    of a random Bernoulli event is the distance from the dashed line,
    Pₚ(ξ)=|ξ−(1−p)|. Consequently, the information content is
    Iₚ(ξ)=−ln|ξ−(1−p)|, and the average information content of the
    sequence is<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-8-56106015">8</a></p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd89ef81a-20f7-406d-b6f8-f45912fe2c26_1224x214.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd89ef81a-20f7-406d-b6f8-f45912fe2c26_1224x214.png" /></a></p>
    <p>Why am I bringing this up? <a
    href="https://en.wikipedia.org/wiki/Sanov%27s_theorem">It turns
    out</a> that the rate function J(ξ) is the solution to <a
    href="https://en.wikipedia.org/wiki/Freidlin%E2%80%93Wentzell_theorem">an
    optimal control problem</a> that involves the <strong>average excess
    surprise</strong>. In particular<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-9-56106015">9</a>,</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9a9c996-9ecd-4bcf-a759-bba0e4215a94_910x124.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9a9c996-9ecd-4bcf-a759-bba0e4215a94_910x124.png" /></a></p>
    <p>The intuition behind this equation is: If we want to know the
    rate function for an extreme event, [x₁,…,xₙ], we get to pick how
    the extreme event comes about (that’s the inf). The only thing we
    have to respect is that the extreme event <em>does,</em> in fact,
    come about</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd85fc50-6030-4853-afd6-df7c228ac56b_618x160.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd85fc50-6030-4853-afd6-df7c228ac56b_618x160.png" /></a></p>
    <p>And then we try to make it so that the <a
    href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">average
    excess surprise</a></p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6752fc7-3814-4351-81e4-56effb2be88e_1752x158.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6752fc7-3814-4351-81e4-56effb2be88e_1752x158.png" /></a></p>
    <p>is as small as possible.</p>
    <p>Let’s see how this plays out in our house deal example. We want
    to determine the probability that the sum of independent random
    variables ξ is less than some success fraction ξ times N,</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80a2a01e-05f8-447c-857d-b6af6e3ac833_830x156.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F80a2a01e-05f8-447c-857d-b6af6e3ac833_830x156.png" /></a></p>
    <p>i.e. that a certain fraction 1−ξ of our N house deals go
    badly.</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fce6049f8-0fd6-4855-8df2-a07d11e3f31b_1857x895.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fce6049f8-0fd6-4855-8df2-a07d11e3f31b_1857x895.png" /></a></p>
    <p>Now <em>we</em> pick how the extreme event, [x1,…,xₙ], comes
    about. I went ahead and already did that in the illustration. I
    picked it so that the first (1−ξ)N deals go wrong, and only the
    remaining ξN deals go fine. This satisfies the constraint that</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F142a13c2-4a8d-4dc6-8b69-6535197447ae_632x170.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F142a13c2-4a8d-4dc6-8b69-6535197447ae_632x170.png" /></a></p>
    <p>and (thanks to commutativity<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-10-56106015">10</a>)
    the average excess surprise of this configuration is identical to
    all possible permutations. Therefore, this configuration’s average
    excess surprise is already the infimum of all possible
    configurations that satisfy the constraint. Given the expression we
    wrote above for the average information content of a Bernoulli
    sequence, we can write the average excess surprise as</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe7c05a0-345e-4f35-89c1-37e848ad418b_1988x256.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe7c05a0-345e-4f35-89c1-37e848ad418b_1988x256.png" /></a></p>
    <p>Since we know that the first ξN many xᵢ are equal to 1 and that
    the remaining (1-ξ)xᵢ are equal to 0, we can simplify the expression
    for J(ξ) by splitting up the sums and regrouping,</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb84a92ab-f2d8-42dd-915a-cf23ec9c8c73_1140x230.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb84a92ab-f2d8-42dd-915a-cf23ec9c8c73_1140x230.png" /></a></p>
    <p>You might recognize this as the KL-divergence between two
    Bernoulli random variables, DKL(Ξ||X)=E(ΞlnΞX), one with success
    probability p, the other with success probability ξ. When we plug in
    the extreme event that <em>all</em> the deals go poorly, ξ=0, the
    rate function reduces to J(ξ)=−ln(1−p), and the probability of this
    extreme event is</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F57208c2b-fad3-44ef-ad6e-03a80d52400b_1576x290.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F57208c2b-fad3-44ef-ad6e-03a80d52400b_1576x290.png" /></a></p>
    <p>as it should be.</p>
    <h1 id="an-adversarial-environment"><strong>An adversarial
    environment</strong></h1>
    <p>Quick recap: 1. When we find ourselves in a random environment,
    we can leverage the law of large numbers to push the probability of
    failure close to zero by trying sufficiently many times<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-11-56106015">11</a>,
    N. 1. If we want to determine <em>how fast</em> the probability of
    failure goes to zero, we can find that out by solving an optimal
    control problem where we need to determine the least surprising way
    failure might come about, J(ξ). 1. The probability of failure goes
    to zero exponentially fast, proportional to how surprised we would
    be and the number of tries, exp(−NJ(ξ)).</p>
    <p>So far, so good, but… what is that sound in the distance<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-12-56106015">12</a>?</p>
    <p>As general as the theorems mentioned above are in terms of the
    underlying distributions<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-13-56106015">13</a>,
    they <strong>centrally rely on the assumption that the environment
    is</strong> _ <strong>random</strong>_. This was not the case for
    Zillow, where homeowners and real estate agencies selling houses had
    <a href="https://en.wikipedia.org/wiki/Caveat_emptor">additional
    information</a> to systematically select bad deals for Zillow.
    Instead of pushing the risk of total failure strongly toward zero,
    Zillow only increasingly exposed itself to an adversary with more
    insight into the situation.</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fecb938db-bb6c-408f-b212-cef5f0f1d787_1162x1311.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fecb938db-bb6c-408f-b212-cef5f0f1d787_1162x1311.png" /></a></p>
    <p>While the <strong>probability of complete failure,</strong>
    exp(−NJ(ξ)), <strong>falls exponentially</strong> , the
    <strong>amount of surprise required to bring it about</strong> ,
    NJ(ξ), <strong>scales linearly!</strong> A few things of note: 1.
    The unit of “surprise” is <a
    href="https://en.wikipedia.org/wiki/Nat_(unit)">nats</a>, a measure
    of information. This interpretation maps only weakly onto the house
    buying example (where one <a
    href="https://rdrr.io/cran/infotheo/man/natstobits.html#:~:text=Details,log2(e)%20%3D%201.442695.">bit</a>
    of information might indicate whether a given deal is bad for
    Zillow). Still, it has a much more straightforward interpretation
    when we think about set-ups where <a
    href="https://en.wikipedia.org/wiki/AI_box">information passes
    through a single channel and can be arbitrarily constrained</a>. 1.
    Similarly, we can interpret the expected excess surprise,</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcafc3e71-9dac-4cb5-93a3-4fe801bb5949_1136x122.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fcafc3e71-9dac-4cb5-93a3-4fe801bb5949_1136x122.png" /></a></p>
    <p>, as a functional on a state space spanned by [x₁,…,xₙ]. The fact
    that we compute the infimum over this functional turns the problem
    into an instance of <a
    href="https://en.wikipedia.org/wiki/Hamilton%27s_principle">Hamilton’s
    principle</a> - the “true” evolution of the sequence of states is
    stationary w.r.t. the “action” given by the excess surprise. Through
    this lens, we’re close to “<a
    href="https://www.alignmentforum.org/tag/really-powerful-optimization-process">intelligence
    as optimization power</a>”, which characterizes intelligence as the
    ability to steer reality into a small subset of possible states. But
    I can’t quite put the pieces in the right place to make the
    connection click. 1. Since we compute NJ(ξ) as the solution to an
    optimal control problem that minimizes excess surprise, <strong>it
    is the lower bound on the amount of surprise required</strong>.
    Catastrophe can also come about in more surprising ways, but <a
    href="https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-30.pdf">highly
    improbable events tend to happen in the least improbable way</a>. On
    the plus side, catastrophe <em>cannot</em> come about with less
    surprise, which might have applications for the <a
    href="https://en.wikipedia.org/wiki/AI_box">set-ups with constrained
    communication</a> mentioned in point 1. 1. The team at Redwood
    Research recently <a
    href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood#:~:text=With%20a%20conservative,rate%20of%20injuriousness).">released
    their report on their research on high-stakes alignment</a>, where
    they trained a filter to reduce the probability of harmful output
    from 2.5% in the training set to 0.003% after applying the filter to
    0.002% after adversarial training. While OOM reduction in failure
    probability <em>feels</em> comforting, this only helps when the
    adversary is not actively trying to trick the filter (from 3.6 nats
    to 10.8 nats of information). This is consistent with their finding
    that their tool-assisted search for adversarial examples was 1000x
    more effective than naive sampling.</p>
    <p>These factors combined make me excited about exploring this
    direction further to find reliable estimates of the lower bound of
    input required to bring about catastrophe<a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-14-56106015">14</a>.
    I’m also curious if there is more structure to the similarity
    between the information content of an adversarial attack and the
    characterization of intelligence in terms of optimization processes.
    If you know more about this or can point me to some literature on
    the topic, please let me know!</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-1-56106015">1</a></p>
    <p>(appropriately named)</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-2-56106015">2</a></p>
    <p>How much is $420 million again? My gut feeling is that this type
    of snafu happens somewhere in the world every other week.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-3-56106015">3</a></p>
    <p>In the same way that the maximum of a set of random variables <a
    href="https://universalprior.substack.com/p/slightly-advanced-decision-theory?s=w#:~:text=is%20coming%20from.-,Diverse%20detours.,-On%20the%20topic">can
    be overwhelmingly good in expectation</a>, the minimum of the same
    set is terrible (in expectation). That’s the power of choice.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-4-56106015">4</a></p>
    <p>Well, not always. That’s the point of this post.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-5-56106015">5</a></p>
    <p>you ask, presumably on the edge of your seat.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-6-56106015">6</a></p>
    <p>(you need the logarithm of the <a
    href="https://en.wikipedia.org/wiki/Moment-generating_function">moment
    generating function</a> of your random variable, λ(θ)=lnE(exp(θXi)),
    and then you need to compute the Legendre-Fenchel transformation,
    J(ξ)=supθ&gt;0[θξ−λ(θ)].)</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-7-56106015">7</a></p>
    <p>I don’t have a reference that follows the same path I choose, but
    I enjoy the texts by <a
    href="https://arxiv.org/abs/1106.4146">Touchette 2011</a> and <a
    href="https://www.google.com/search?q=paninski+large+deviation+optimal+control+leaky+integrate&amp;oq=paninski+large+deviation+optimal+control+leaky+integrate&amp;aqs=chrome..69i57j0i512l2j46i512j0i10i30j0i30j0i5i30l4.11018j0j4&amp;sourceid=chrome&amp;ie=UTF-8#:~:text=The%20most%20likely%20voltage%20path%20and%20large%20deviations,stat.columbia.edu%20%E2%80%BA%20pubs%20%E2%80%BA%20ml%2Dpath%2Djcns">Paninski
    2006</a>.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-8-56106015">8</a></p>
    <p>Absolute values are a bit icky, but we can be sneaky since we
    have the logarithm and write</p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa439e97d-2860-460c-99f2-7fac4d0f5198_986x174.png"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa439e97d-2860-460c-99f2-7fac4d0f5198_986x174.png" /></a></p>
    <p>instead. This expression now has an uncanny similarity to the <a
    href="https://en.wikipedia.org/wiki/Freidlin%E2%80%93Wentzell_theorem">Freidlin-Wentzell</a>
    rate function, which can’t be a coincidence.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-9-56106015">9</a></p>
    <p>(I recommend staring at that for a while.)</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-10-56106015">10</a></p>
    <p>When the control problem gets more involved and the individual
    time points are not interchangeable, this step gets a bit trickier,
    and we have to recruit some <a
    href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">mathematical
    machinery</a>.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-11-56106015">11</a></p>
    <p>Given that the expected value is positive, ofc.</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-12-56106015">12</a></p>
    <p><a
    href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa03ec4ae-9bee-469e-bac3-065fdeae7b60_3000x4500.jpeg"><img
    src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fa03ec4ae-9bee-469e-bac3-065fdeae7b60_3000x4500.jpeg" /></a><a
    href="http://www.blackswanman.com/#lg=1&amp;slide=0">source</a></p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-13-56106015">13</a></p>
    <p>and any additional structure you might want to slap on</p>
    <p><a
    href="https://universalprior.substack.com/p/adversarial-attacks-and-optimal-control#footnote-anchor-14-56106015">14</a></p>
    <p>Perhaps we can manage to stay below that bound reliably?</p>
    <div class="debug-grid"></div>
  <script src="index.js"></script>
</body>

</html>