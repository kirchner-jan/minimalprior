<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="" >

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="Jan Kirchner" />
      <meta name="dcterms.date" content="2023-01-03" />
        <title>minimalprior</title>
    <link rel="stylesheet" href="../../reset.css" />
    <link rel="stylesheet" href="../../index.css" />
      </head>

<body>
    <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title"><a href="https://kirchner-jan.github.io/minimalprior/"
            style="text-decoration: none; color: inherit;">minimalprior</a></h1>
        <span class="subtitle">a spinoff</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">2023-01-03</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://universalprior.substack.com/">Jan
Kirchner</a></td>
    </tr>
  </table>
      <nav id="TOC" role="doc-toc">
        <ul class="incremental">
        <li><a href="#nerding-out-with-the-og-cyborgs"
        id="toc-nerding-out-with-the-og-cyborgs">Nerding out with the OG
        cyborgs</a></li>
        <li><a href="#semiotic-coin-flip"
        id="toc-semiotic-coin-flip">Semiotic coin flip</a></li>
        <li><a href="#closing-thoughts"
        id="toc-closing-thoughts">Closing thoughts</a></li>
        </ul>
  </nav>
    <h2 id="nerding-out-with-the-og-cyborgs">Nerding out with the OG
    cyborgs</h2>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_197.png" /></p>
    <p>A few months ago, the <a
    href="https://twitter.com/repligate/status/1609712964705157123?s=20&amp;t=nd8IRj_AuWyqZ4NsToSQTQ">enigmatic</a>
    <a href="https://generative.ink/prophecies/">janus</a> published a
    write-up on what they call “<a
    href="https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators">simulator
    theory</a>” on the Alignment Forum. The post spurred a lot of
    discussions and changed a bunch of minds. This outcome was not
    completely clear from the onset - janus has been a fixture in the
    alignment community and <a href="https://www.eleuther.ai/">on
    Discord</a>, and their thought has been <a
    href="https://astralcodexten.substack.com/p/janus-gpt-wrangling">in
    the water</a> for a while. And the message of the simulator post is
    comparatively simple - “large language models can/should be viewed
    as simulators”. But the post ended up being a big success; it turns
    out that having janus’ philosophy laid out in writing enabled a lot
    more structured discussion than what would have been possible
    otherwise.</p>
    <p>One of the structured discussions that were enabled by the post
    was a seminar series that I got to participate in over the last few
    months. <a
    href="https://www.alignmentforum.org/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared">Our
    motivation</a> for having the seminar was that simulator theory (in
    its current form) is still in its infancy. To me, the prototypical
    example of a successful theory is decision theory, which went
    through the <a
    href="https://universalprior.substack.com/i/46914010/how-to-solve-chess">full
    stack</a>:</p>
    <ul class="incremental">
    <li><p>from philosophy (moral philosophy, utilitarianism, “what
    should I do?”),</p></li>
    <li><p>to mathematics (mathematical decision theory, game theory,
    “which equations capture optimal decisions?”),</p></li>
    <li><p>to computer science (reinforcement learning, evolutionary
    algorithms, “which algorithms approximate optimal
    decisions?”).</p></li>
    </ul>
    <p>When compared to decision theory, simulator theory looks like
    it’s still in the early philosophy stage, with a lot of questions
    unanswered (and unasked). Making progress at pushing simulator
    theory through the stack from theory to practice was perhaps the
    central intention that is shared among all participants of the
    seminar.</p>
    <p>However, as we learned through the seminar, janus has indeed
    asked and answered many more questions than they managed to present
    in the <a
    href="https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators">original
    simulator theory</a> post. Beyond the initial observations that “<a
    href="https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators#Simulators">it
    is useful to think of GPT as a simulator</a>” and “<a
    href="https://www.alignmentforum.org/posts/vJFdjigzmcXMhNTsx/simulators#Inadequate_ontologies">simulators
    have a few beneficial properties that we might not expect from
    agents/oracles/tools</a>”, janus has figured out a bunch of things
    about the inner workings of simulators, and about the ontologies
    induced by simulators. As a consequence, we spent a lot of time in
    the seminar trying to catch up with janus’ thought process.</p>
    <p><a
    href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/tspsis-and-how-i-write">The
    problem</a> is that janus (who are to admit this) take a long time
    to write things up. Not quite G.R.R.-Martin-level bad, but given
    that we might get transformative AI before “<a
    href="https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire#A_Dream_of_Spring">A
    Dream of Spring</a>”, the differences in speed are negligible. Thus,
    we decided to make a series of write-ups on our learnings from the
    seminar, to speed up the development of simulator theory and move it
    from philosophy to mathematics as much as possible.</p>
    <p>Today we published the <a
    href="https://www.alignmentforum.org/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared">first</a>
    <a
    href="https://www.alignmentforum.org/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics">two</a>
    posts in the sequence! In case you want to get the full scoop, I
    invite you to check them out now. If you’re more here for the 20-80,
    I reproduce one of the coolest results from the first session below
    - the semiotic coin.</p>
    <h2 id="semiotic-coin-flip">Semiotic coin flip</h2>
    <p>The first session of the seminar was concerned with what janus
    calls “semiotic physics”. Semiotics is the study of signs and
    symbols and their use or interpretation. Physics is physics. But the
    type of physics we were discussing at the seminar is the type that
    tries to <a
    href="https://www.lesswrong.com/tag/reductionism-sequence">explain
    everything in the universe with simple rules</a>. Semiotic physics,
    then, is the <em>study of the fundamental forces and laws that
    govern the behavior of signs and symbols</em>.</p>
    <p>This description makes it sound very high-strung, so let me
    illustrate with a toy model that we are familiar with from regular
    physics: coin flips. In this setup, we draw a sequence of coin flips
    from a large language mode<sup>[1]</sup>. We encode the coin flips
    as a sequence of the strings <code>1</code> and <code>0</code>
    (since they are tokenized as a single token) and zero out all
    probabilities of other tokens.</p>
    <div class="sidenote">
    <p>[1] </p>
    <p>The figures are generated with data from OpenAI’s ada model, but
    the same principle applies to other models as well.</p>
    </div>
    <p>We can then look at the probability of the event E that the
    sequence of coin flips ends in tails (<code>0</code>) or heads
    (<code>1</code>) as a function of the sequence length.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_198.png" /></p>
    <p>We note two key differences between the semiotic coin flip and a
    fair coin:</p>
    <ul class="incremental">
    <li><p>the semiotic coin is not fair, i.e. it tends to produce
    sequences that end in tails (<code>0</code>) much more frequently
    than sequences that end in heads (<code>1</code>).</p></li>
    <li><p>the semiotic coin flips are not independent, i.e. the
    probability of observing heads or tails changes with the history of
    previous coin flips.</p></li>
    </ul>
    <p>To better understand the types of sequences that end in either
    tails or heads, we next investigate the probability of the most
    likely sequence ending in <code>0</code> or <code>1</code>. As we
    can see in the graph below, the probability of the most likely
    sequence ending in <code>1</code> does not decrease for the GPT coin
    as rapidly as it does for a fair coin.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_199.png" /></p>
    <p>Again, we observe a notable difference between the semiotic coin
    and the fair coin:</p>
    <ul class="incremental">
    <li>while the probability of a given sequence of coin flips
    decreases exponentially (every sequence of length T of fair
    coinflips has the same probability</li>
    </ul>
    <p>\(\)</p>
    <p>the probability of the most likely sequence of semiotic coin
    flips decreases much slower.</p>
    <p>This difference is due to the fact that the most likely sequence
    of semiotic coinflips ending in f.e. <code>0</code> is:
    <code>0</code> <code>0</code> <code>0</code> <code>0</code> …
    <code>0</code> <code>0</code>. Once the language model has produced
    the same token four or five times in a row, it will latch onto the
    pattern and continue to predict the same token with high
    probability. As a consequence, the probability of the sequence does
    not decrease as drastically with increasing length, as each
    successive term has almost a probability of 1.</p>
    <p>Avid readers of this blog will recognize the set-up we are
    preparing here as the <a
    href="https://kirchner-jan.github.io/minimalprior/posts/universalprior/adversarial-attacks-and-optimal-control">set-up
    for a large deviation principle</a>. And indeed, it turns out that
    we can indeed predict the probability of a sequence of coin flips
    ending in 0 by determining the probability of the <em>most
    likely</em> sequence of coin flips ending in 0.</p>
    <p><img
    src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_200.png" /></p>
    <p>Here we see that, indeed, the negative probability of the most
    likely sequence from E scales as</p>
    <p>\((E).\)</p>
    <p>just as we would expect from a large deviation principle. The
    full mathematical shebang to formulate this proposition is <a
    href="https://www.alignmentforum.org/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics">in
    the forum post</a>, but even there we don’t have a mathematically
    rigorous proof. But that was part of the beauty of the seminar; by
    limiting the time we spend per session, we forced ourselves not to
    get hung up on details.</p>
    <h2 id="closing-thoughts">Closing thoughts</h2>
    <p>Attending this seminar was one of my clear highlights of the past
    months. Discussing the shape and function of simulators with a group
    of people who share a lot of interests, but also contribute their
    diverse backgrounds, was a constant string of revelations. I am
    still confused about a lot of things, and there are a bunch of
    experiments I can’t wait to run, but even if none of the wild ideas
    we churned through end up being correct, I’m still bullish on the
    overall format of the seminar.</p>
    <div class="debug-grid"></div>
  <script src="index.js"></script>
</body>

</html>