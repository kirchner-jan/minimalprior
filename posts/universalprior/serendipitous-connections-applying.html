<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Jan Kirchner" />
  <meta name="dcterms.date" content="2022-01-04" />
  <title>Serendipitous connections applying explanations from AI to the brain</title>
  <link rel="stylesheet" href="../../reset.css" />
  <link rel="stylesheet" href="../../index.css" />
</head>

<body>
  <table class="header">
    <tr>
      <td colspan="2" rowspan="2" class="width-auto">
        <h1 class="title"><a href="https://kirchner-jan.github.io/minimalprior/"
            style="text-decoration: none; color: inherit;">Serendipitous
            connections applying explanations from AI to the brain</a></h1>
        <span class="subtitle">TL;DR A small shift in perspective helps
          interpret the ventral stream in the biological brain as the residual
          stream from ResNets. Experimental…</span>
      </td>
      <th>Updated</th>
      <td class="width-min"><time style="white-space: pre;">2022-01-04</time></td>
    </tr>
    <tr>
      <th class="width-min">Author</th>
      <td class="width-auto"><a href="https://kirchner-jan.github.io/minimalprior/">Jan
          Kirchner</a></td>
    </tr>
  </table>
  <nav id="TOC" role="doc-toc">
    <ul class="incremental">
      <li><a href="#visionary-hierarchy" id="toc-visionary-hierarchy"><strong>Visionary
            Hierarchy</strong></a></li>
      <li><a href="#not-the-full-story" id="toc-not-the-full-story"><strong>Not the full
            story</strong></a></li>
      <li><a href="#the-residual" id="toc-the-residual"><strong>The
            Residual</strong></a></li>
      <li><a href="#put-it-together-it-just-makes-sense" id="toc-put-it-together-it-just-makes-sense"><strong>Put it
            together, it just makes sense</strong></a></li>
    </ul>
  </nav>
  <p>I recently read the new <a href="https://transformer-circuits.pub/">Transformers Circuit</a>
    work by <a href="https://transformer-circuits.pub/2021/framework/index.html">Elhage
      et al.</a> at <a href="https://www.anthropic.com/">Anthropic</a>.
    While I am in no position to judge the importance of the paper for
    AI, (not that this has ever stopped me from sharing my opinion
    before) the paper made me connect some ideas from neuroscience that
    I couldn’t connect before. This post is capturing those connections.
    I’m not sure if all of this is novel<sup>[1]</sup>, but it was novel
    to me!</p>
  <div class="sidenote">
    <p>[1] </p>
    <p>I fully expect someone to tell me: Oh, that’s the
      “The-Brain-Is-A-Residual-Stream” theory from Smith et al. 1986.</p>
  </div>
  <p>If you just want to read the idea, feel free to skip to the
    section <strong>“Put it together, it just makes sense”</strong>. If
    you know a bit of neuroscience, but no machine learning, skip to the
    section <strong>“The Residual”</strong>. If you know a bit of
    machine learning, but no neuroscience, continue with the sections
    <strong>“Visionary Hierarchy”</strong> and <strong>“Not the full
      story”</strong>. If you want <em>everything</em> , just keep
    reading.
  </p>
  <h2 id="visionary-hierarchy"><strong>Visionary
      Hierarchy</strong></h2>
  <p>There is this idea floating around since at least <a
      href="https://braintour.harvard.edu/archives/portfolio-items/hubel-and-wiesel">Hubel
      and Wiesel</a> that information processing in the brain is
    <em>hierarchical</em> , in the sense that there are “lower” and
    “higher” levels of information processing. I will call this broad
    idea the “naive hierarchical brain theory” or <strong><a href="https://newhorizonsbandtoronto.ca/">NHBT</a></strong>
    to
    indicate that probably very few people believe <em>exactly</em>
    this.
  </p>
  <p><strong>Ventral stream.</strong> One particularly well-studied
    example of the NHBT is the “<a href="https://neuroscientificallychallenged.com/glossary/ventral-stream">ventral
      stream</a>”:</p>
  <blockquote>
    <p>a pathway that carries visual information from the primary visual
      cortex to the temporal lobe.</p>
  </blockquote>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_93.png" />Hierarchical
    processing in the ventral stream. (<a href="https://jov.arvojournals.org/article.aspx?articleid=2193828">Manassi
      et al 2013</a>)</p>
  <p>The ventral stream is concerned with extracting properties of
    objects from the “raw input” from the retina. The NHBT suggests
    that</p>
  <ol class="incremental" type="1">
    <li>after starting from roundish, simple representations in the
      lateral geniculate nucleus (LGN),</li>
    <li>we get raw edges at the level of the primary visual cortex
      (V1),</li>
    <li>we move to flat shapes in the secondary visual cortex (V2),</li>
    <li>we skip V3<sup>[2]</sup>,</li>
    <li>and finally on to the quaternary visual cortex (V4) for
      three-dimensional objects.</li>
  </ol>
  <p>In animals that care about faces, we then move on to the inferior
    temporal cortex (IT) to represent faces and <a href="https://en.wikipedia.org/wiki/Grandmother_cell">Jennifer
      Aniston, in particular</a>. Once we’ve reached this very high-level
    representation, we might implement certain high-level behavioral
    strategies like</p>
  <pre><code>see tiger -&gt; run away</code></pre>
  <p>or</p>
  <pre><code>see grandmother -&gt; don&#39;t run away</code></pre>
  <p><strong>Building neural representations.</strong> A central idea
    of the NHBT is that higher-level representations are “assembled”
    from lower-level representations. In particular, the edges detected
    in the primary visual cortex are assembled from smaller, circular
    representations in the LGN.</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_94.png" />LGN
    receptive fields (orange blobs on the left) are pooled from multiple
    LGN cells to a V1 cell (right) and form a spatially-elongated,
    edge-selective receptive field (dashed line on the left).</p>
  <p>This particular circuit has already been proposed by <a
      href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/">Hubel
      and Wiesel in 1962</a> and has decent experimental evidence (<a
      href="https://pubmed.ncbi.nlm.nih.gov/7477347/">Reid and Alonso,
      1995</a>;<a href="https://www.jneurosci.org/content/37/21/5250">Sedigh-Sarvestani
      et al, 2017</a>). We also understand a lot about the cortical
    dynamics of how this comes about from smart electrophysiology
    experiments (<a href="https://www.sciencedirect.com/science/article/pii/S0896627301002781#BIB32">Lampl
      et al., 2001</a>), from looking at all the inputs of pyramidal
    neurons (<a href="https://www.nature.com/articles/s41586-020-2894-4">Rossi et
      al., 2020</a>), and by modeling this to death (<a href="https://pubmed.ncbi.nlm.nih.gov/9425519/">Carandini and
      Ringach, 1997</a><sup>[3]</sup>).</p>
  <div class="sidenote">
    <p>[3] </p>
    <p>And a lot more. Even your’s truly has a model that can do this!
      (<a href="https://www.nature.com/articles/s41467-021-23557-3">Kirchner
        and Gjorgjieva, 2021</a>).</p>
  </div>
  <p>After getting orientation selectivity in V1 there is less
    agreement about how to get shapes, objects, and faces in the
    higher-order cortices. My favorite work on this topic comes from <a
      href="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/GOMES1/marr.html">David
      Marr (1982)</a>, but I can’t find super strong experimental evidence
    that teases apart a circuit implementation<sup>[4]</sup>.</p>
  <div class="sidenote">
    <p>[4] </p>
    <p>This looks relevant however,<a href="https://www.cell.com/neuron/fulltext/S0896-6273%2811%2900995-0">Verhoef
        et al., 2012</a>.</p>
  </div>
  <p><strong>Canonical circuits for assembling
      representations.</strong> However, perhaps an exact implementation
    is also not necessary. There is a hand-wavy, standard circuit-level
    explanation referred to as the “canonical microcircuit” (<a
      href="https://www.frontiersin.org/articles/10.3389/fnana.2011.00030/full">Shepherd,
      2011</a>). In its simplest<sup>[5]</sup> form, the theory describes
    the information flow through the six layers of the cortex. Lower
    levels (f.e. LGN) project to neurons in layer 4, from where the
    information gets forwarded to layer 2/3<sup>[6]</sup>, from there
    back to layer 5, and from there to the next level in the
    hierarchy.</p>
  <div class="sidenote">
    <p>[5] </p>
    <p>i.e. only reasonably useful,</p>
  </div>
  <div class="sidenote">
    <p>[6] </p>
    <p>If you wonder why there is no mention of layer 1 or layer 6, and
      why layers 2 and 3 are lumped together - that is <em>also</em> the
      fault of<a
        href="https://en.wikipedia.org/wiki/Cerebral_cortex#:~:text=The%20work%20of%20Korbinian%20Brodmann%20%281909%29%20established%20that%20the%20mammalian%20neocortex%20is%20consistently%20divided%20into%20six%20layers">that
        guy in 1909</a>…</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_95.png" />Schematic
    of the six layers of the cortex. Eight sample neurons are displayed
    that receive input from the previous level of the hierarchy (blue),
    perform recurrent processing (black), and output the result to the
    next level of the hierarchy (brown). Based on <a
      href="https://www.sciencedirect.com/science/article/pii/S0301008218301369">Kast
      and Levitt, 2019</a>.</p>
  <p>We can therefore think of layer 4 as the “input” layer and layer
    5 as the “output” layer. Cells in layer 2/3 are highly
    interconnected and form a type of <a href="https://en.wikipedia.org/wiki/Reservoir_computing">reservoir
      for performing computations</a>. Exactly what kind of computation
    this circuit performs is still under debate, but there are plenty of
    ideas floating around (see f.e. <a href="https://www.nature.com/articles/s41583-020-00390-z">Sadeh and
      Clopath, 2021</a>). In the NHBT, layer 2/3 might be responsible for
    combining lower-level representations to form higher-level
    representations.</p>
  <p><strong>Strength of the NHBT.</strong> There is also a lot of
    inspired work at the intersection between machine learning and
    neuroscience, where the representations that emerge in deep
    artificial neural networks are compared with those in the brain (<a
      href="https://www.pnas.org/content/111/23/8619.short">Yamins et al.,
      2014</a>). And, almost fully in the land of AI, there is the
    Circuits work by <a href="https://distill.pub/2020/circuits/zoom-in/">Olah et
      al. (2020)</a> that “zooms in” on different levels in a deep
    convolutional network to identify and interpret the motifs that give
    the network its umpf<sup>[7]</sup>.</p>
  <div class="sidenote">
    <p>[7] </p>
    <p><a
        href="https://www.urbandictionary.com/define.php?term=umpf#:~:text=Power%2C%20Strength%2C%20Energy%2C%20Might%2C%20Force%2C%20Potency%2C%20Ability%2C%20Excite">This</a>
      one, not<a
        href="https://www.urbandictionary.com/define.php?term=umpf#:~:text=A%20word%20that%20represents%20the%20sound%20made%20when%20one%20craves%20sexual%20intercourse.">this</a>
      one.</p>
  </div>
  <p>The whole story is very appealing for many reasons:</p>
  <ul class="incremental">
    <li>
      <p>It’s a simple theory with lots of experimentally-testable
        predictions.</p>
    </li>
    <li>
      <p>It resonates nicely with the philosophy of <a
          href="https://www.lesswrong.com/posts/tPqQdLCuxanjhoaNs/reductionism">reductionism</a>.</p>
    </li>
    <li>
      <p>It goes back to Hubel and Wiesel, which is a kind of magic
        incantation that <a
          href="http://jwilson.coe.uga.edu/EMT668/EMAT6680.F99/Challen/proof/proof.html#:~:text=which%20it%20is.-,Proof%20by%20authority,-%3A%20%22Well%2C%20Don%20Knuth">bestows
          credibility</a>.</p>
    </li>
  </ul>
  <h2 id="not-the-full-story"><strong>Not the full story</strong></h2>
  <p>I’m not saying that anybody is saying that the NHBT is the full
    story<sup>[8]</sup>. But it’s certainly an influential story that a
    lot of people take as a starting point for building more complicated
    models. <em>I’m</em> certainly using the NHBT a lot when I need
    high-level intuitions for how sensory processing works. However,
    there are also a bunch of puzzle pieces that I’ve collected over the
    years that don’t fit at all.</p>
  <div class="sidenote">
    <p>[8] </p>
    <p>I’d perhaps argue that the situation is even worse and that
      people don’t have a theory concrete enough to be wrong, but that
      would just bog us down at this point.</p>
  </div>
  <p><strong>Redundancies.</strong> I’ve laid out above how the
    representation of edges emerges in the primary visual cortex from
    combining simpler features from the LGN. Well, it turns out that 35%
    of retinal ganglion cells in the <strong>eye</strong> already encode
    edges (<a href="https://www.nature.com/articles/nature16468">Baden
      et al., 2016</a>).</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_5.gif" />A
    schematic of a retinal ganglion cell with bars moving across it to
    indicate visual flow. Black bars on the side indicate the generated
    action potentials of the cell. Note how only certain directions of
    the bars induce substantial action potential firing. Stolen from <a
      href="https://www.youtube.com/watch?v=AC9nSsFd-J4&amp;ab_channel=CellPress">Rivlin-Etzion
      et al., 2012</a>.</p>
  <p>In fact, researchers are constantly trying to one-up each other
    in the number of different things that are encoded already at the
    retina (spatial contrast, color, motion, flicker, fine and coarse
    textures, absolute light level, … <a href="https://www.frontiersin.org/articles/10.3389/fneur.2021.661938/full">Kim
      et al., 2021</a>). Note that this is <em>before</em> the stimulus
    has even reached the LGN. In fact, the <em>same</em> mechanism that
    computes the direction of moving edges in the cortex (<a
      href="https://www.nature.com/articles/s41586-020-2894-4">Rossi et
      al., 2020</a>) has been identified a decade earlier in the retina
    (<a href="https://www.nature.com/articles/nature09818">Briggman et
      al., 2011</a>).</p>
  <p>From the NHBT perspective, this is just wasteful! Why would you
    compute a representation of edges already at the earliest stage of
    your hierarchy and then do the whole thing <em>again</em> three
    steps later? Even worse, it turns out that at later stages in the
    ventral pathway, there are <em>still</em> cells that only encode
    edges (<a href="https://www.sciencedirect.com/science/article/pii/S0896627313005266">Olcese
      et al., 2013</a>). There are ways we can try to explain this
    away<sup>[9]</sup>, but prima facie this is just inconsistent with
    the naive perspective.</p>
  <div class="sidenote">
    <p>[9] </p>
    <p>Maybe the cortex is just doing this <em>better</em> than the
      retina? Perhaps the computation in the retina is for rapidly
      filtering out useless input that doesn’t even need to be sent to the
      cortex? After all, there is a bottleneck (the LGN has much fewer
      neurons than the retina or the primary visual cortex) that the
      information needs to be passed through.</p>
  </div>
  <p><strong>Input, Output, all the same.</strong> Also, get this: The
    supposed “output” layer 5 of the cortex <em>also</em> receives the
    same input as the “input” layer 4 (<a href="https://www.science.org/doi/abs/10.1126/science.1236425">Constantinople
      and Bruno, 2013</a>)! If you carefully combine your lower-level
    representation to a higher-level representation through the layer
    2/3 -&gt; layer 4 -&gt; layer 5 pathway, why would you slap on the
    original, lower-level representation in layer 5 again?</p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_96.png" />As
    the schematic before, but now with an additional “shortcut”
    connection that connects the input directly to the output unit.</p>
  <p>There are a lot of other connections<sup>[10]</sup> that are not
    drawn into this diagram, but none of them are quite as strikingly
    inconsistent with the NHBT<sup>[11]</sup>.</p>
  <div class="sidenote">
    <p>[10] </p>
    <p>My favorite resource comes from the Allen Brain Institute<a
        href="https://portal.brain-map.org/explore/models/mv1-all-layers">here</a>.</p>
  </div>
  <div class="sidenote">
    <p>[11] </p>
    <p>In particular, each transmission from one cell to another induces
      a temporal delay of a few milliseconds. This might be negligible for
      most macroscopic behavior, but for neural circuits <a
        href="http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity">timing
        matters</a>. Directly feeding the input into the output messes this
      up, big time.</p>
  </div>
  <p><strong>The problem with grandma.</strong> Remember that thing
    about Jennifer Aniston and the grandmother from the previous
    section? Yeah, turns out that’s super controversial to the point
    where some call it a failure and a myth (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822296/">Barwich,
      2019</a>). Where the NHBT suggests that at the top of the hierarchy
    we find neurons coding for single, complex concepts, the reality in
    the brain looks different. Instead of a cell-to-concept
    correspondence, complex concepts are encoded combinatorially (<a
      href="https://pubmed.ncbi.nlm.nih.gov/18262826/">Quiroga et al.,
      2008</a>):</p>
  <blockquote>
    <p>Neurons might fire selectively in response to highly specific
      stimuli, such as Halle Berry. But this is not the only stimulus to
      which they respond. Additionally, sparse coding does not show the
      encoding of particular stimuli as separate or isolated entities.
      Instead, it builds a <em>network of associations</em> between
      familiar items: While a limited number of neurons responded to
      specific stimuli (say, pictures of Jennifer Aniston), these cells
      also responded selectively to specific stimuli known from the same
      context (namely, Lisa Kudrow, the actress starring next to Aniston
      in the sitcom <em>Friends</em> ). (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6822296/">Barwich,
        2019</a>)</p>
  </blockquote>
  <p>This means, in particular, that it does not make a lot of sense
    to talk about one particular <em>stimulus</em> that is encoded by a
    cell in the higher levels of the hierarchy. Cells need to be
    interpreted in the context of their local network.</p>
  <p>While this is not fully inconsistent with the NHBT where
    representations in the visual cortex are assembled from simpler
    representations in the thalamus<sup>[12]</sup>, it requires an
    additional component to explain why this scheme does not apply to
    the highest levels of the hierarchy.</p>
  <div class="sidenote">
    <p>[12] </p>
    <p>It could be that at lower levels in the hierarchy, the receptive
      field of a cell can still be defined, and only at higher levels, it
      falls apart.</p>
  </div>
  <h2 id="the-residual"><strong>The Residual</strong></h2>
  <p><strong>Degraded performance in deep networks.</strong> We now
    dive briefly into the ancient history of machine learning, all the
    way back into the year 2016<sup>[13]</sup>. Up to this point, steady
    progress in machine learning meant improving relatively
    “shallow”<sup>[14]</sup> networks (&lt; 25 layers) to squeeze out
    more and more performance on the ImageNet classification dataset.
    Intuitively, we might expect deeper networks to perform better (more
    layers should allow more computation), but this didn’t bear out in
    terms of performance. Making networks deeper actually
    <em>degraded</em> performance.
  </p>
  <div class="sidenote">
    <p>[13] </p>
    <p>The field is moving <em>fast</em>.</p>
  </div>
  <div class="sidenote">
    <p>[14] </p>
    <p>People at the time probably did not think these networks were
      “shallow”.</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_97.png" />Schematic
    of the training error as a function of the total number of layers in
    the network. The blue line indicates what <a
      href="https://proceedings.neurips.cc/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html">deep
      learning theory would predict</a>, the red line indicates what was
    observed before the invention of ResNets.</p>
  <p>The reason for this degradation problem, people <a
      href="https://www.fatalerrors.org/a/explain-resnet-in-detail.html#:~:text=there%20is%20a%20problem%20of%20gradient%20vanishing%20or%20exploding%20in%20deep%20network">speculated</a>,
    was that gradients tend to vanish or explode in deeper networks,
    which makes training unstable.</p>
  <p><strong>ResNet to the ResCue.</strong> This changed when the
    ResNet architecture (<a href="https://arxiv.org/abs/1512.03385">He
      et al., 2016</a>) was introduced:</p>
  <blockquote>
    <p>ResNet, short for Residual Networks is a classic neural network
      used as a backbone for many computer vision tasks. This model was
      the winner of ImageNet challenge in 2015. The fundamental
      breakthrough with ResNet was it allowed us to train extremely deep
      neural networks with 150+layers successfully. (<a
        href="https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33">Dvivedi,
        2019</a>)</p>
  </blockquote>
  <p>The <a href="https://www.fatalerrors.org/a/explain-resnet-in-detail.html">central
      intuition</a> for what they did is straightforward: If your shallow
    network achieves good performance, adding additional components
    <em>should</em> just apply the identity transformation and forward
    that good solution to the output. Since they are not doing that,
    they must be struggling to “learn” the identity transformation. We
    can help them by adding a “skip connection” that, by default,
    implements the identity:
  </p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_98.png" />Strongly
    simplified schematic of the ResNet structure. Computations are
    typically nonlinear.</p>
  <p>This worked pretty well, winning a lot of competitions, and
    enabling what Gwern calls the “blessing of scale”:</p>
  <blockquote>
    <p>So, the larger the model, the better, if there is enough data
      &amp; compute to push it past the easy convenient sub-models and
      into the sub-models which express desirable traits like
      generalizing, factorizing perception into meaningful latent
      dimensions, meta-learning tasks based on descriptions, learning
      causal reasoning &amp; logic, and so on. If the ingredients are
      there, it’s going to happen. (<a href="https://www.gwern.net/Scaling-hypothesis">Gwern, 2020</a>)</p>
  </blockquote>
  <p>These skip connections also happen to be one of the central
    components of the transformer architecture (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>)
    that is currently <a href="https://threadreaderapp.com/thread/1468370605229547522.html">revolutionizing
      machine learning</a>.</p>
  <p><strong>Residual stream interpretation.</strong> The “vanishing
    gradient” motivation for using a Resnet is <em>the</em> go-to
    explanation for when and how skip-connections work. <a href="https://youtu.be/1bSPNboKCzM?t=966">There is, of
      course,
      another way of looking at this.</a> Welcome to the stage, <a
      href="https://transformer-circuits.pub/2021/framework/index.html">Elhage
      et al., 2021</a>.</p>
  <p>Rather than interpreting the skip connections as a solution to a
    problem, they evaluate them as an important component of the
    transformer in its own right. In particular, they observe that the
    chaining together of multiple skip connections (a “residual stream”)
    opens the door to a range of interesting new computations. They say
    it much better than I could:</p>
  <blockquote>
    <p>One of the main features of the high level architecture of a
      transformer is that each layer adds its results into what we call
      the “residual stream.” The residual stream is simply the sum of the
      output of all the previous layers and the original embedding. We
      generally think of the residual stream as a communication channel,
      since it doesn’t do any processing itself and all layers communicate
      through it. The residual stream has a deeply linear structure. Every
      layer performs an arbitrary linear transformation to “read in”
      information from the residual stream at the start, and performs
      another arbitrary linear transformation before adding to “write” its
      output back into the residual stream.<sup>[15]</sup></p>
    <div class="sidenote">
      <p>[15] </p>
      <p>Something interesting happened to my mind, where I now cannot
        un-see this interpretation anymore. I feel convinced that something
        about the old “vanishing gradient” explanation was unsatisfactory,
        but that I couldn’t put my finger on it. Hindsight is, of course,
        20/20.</p>
    </div>
  </blockquote>
  <p>Interpreted as a “residual stream”, it is clear that later
    information computed early on is still accessible later in the
    stream<sup>[16]</sup>.</p>
  <div class="sidenote">
    <p>[16] </p>
    <p>As long as the information is not overwritten.</p>
  </div>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_99.png" />Schematic
    illustrating same situation as in the previous figure, but now with
    the skip connections chained together and labeled “residual stream”.
    The thick orange band indicates that the residual stream has a
    limited, but high-dimensional bandwidth.</p>
  <p>In addition, there is a kind of “<a href="https://en.wikipedia.org/wiki/Turing_machine">memory
      management</a>” that becomes possible through the residual stream:
    computational units later in the stream can “pick up and continue”
    the output of previous units, and then copy, modify, delete, or
    merge them. Presented like this, it is clear that the residual
    stream might be more than just a “solution to the problem of
    degrading performance”, and that the residual stream is intimately
    involved in the complex calculations performed by deep networks.</p>
  <h2 id="put-it-together-it-just-makes-sense"><strong>Put it
      together, it just makes sense</strong></h2>
  <p>If you’ve read the previous two sections back-to-back you can
    probably already predict where I’m going: Perhaps we shouldn’t be
    thinking of the steps in the ventral stream as successive levels of
    a <em>hierarchy</em> , but instead, we should think of them as
    reading from and writing into a <em>residual stream</em>. Instead of
    <em>levels</em> in the hierarchy, we think of <em>stations</em> in
    the stream. Instead of <em>merging</em> representations, we
    <em>operate</em> on portions of the stream. It’s just a small shift
    in perspective, but we can get a lot of mileage from it.
  </p>
  <p><img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_100.png" />Ventral
    stream illustrated with a residual stream.</p>
  <p><strong>Model<a href="https://en.wikipedia.org/wiki/Postdiction">postdictions</a>.</strong>
    Under this model,</p>
  <ol class="incremental" type="1">
    <li>
      <p>the original input ought to be added to the computed output,
        consistent with what we know about the direct thalamocortical input
        onto layer 5 of the cortex (<a href="https://www.science.org/doi/abs/10.1126/science.1236425">Constantinople
          and Bruno, 2013</a>). This also happens to be one of the hallmark
        features of the “Dendritic Gated Networks” recently proposed by <a
          href="https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full">Sezener
          et al. (2020)</a>. <img src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_101.png" />As
        schematic before, but with input and output merged into a residual
        stream.</p>
    </li>
    <li>
      <p>representations would still <em>look</em> somewhat
        hierarchical: as later processing steps of the stream receive the
        output of earlier steps as input, complexity<sup>[17]</sup> of the
        representation will increase at later stages in the stream. However,
        the hierarchy is a lot more flexible, and can dynamically route
        intermediate outputs from all levels to the end of the stream. <img
          src="../../images/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32_102.png" />Schematic
        illustrating that later stations in the stream might still have
        access to all previously computed intermediate results.</p>
      <div class="sidenote">
        <p>[17] </p>
        <p>As measured by the degree of processing of the information.</p>
      </div>
    </li>
    <li>
      <p>computation is highly flexible. Depending on the input
        statistics, the same cell might be recruited in a wide range of
        roles.</p>
    </li>
  </ol>
  <p><strong>From receptive fields to sparse coding.</strong>
    Computations early in the stream mostly depend on the input.
    Computations later in the stream depend on both the input and on
    computations earlier in the stream. As a consequence, a cell early
    in the stream has a reliable “receptive field”, while cells later in
    the stream do not strongly bind to the input and can only be
    interpreted as a sparse code (<a href="https://pubmed.ncbi.nlm.nih.gov/18262826/">Quiroga et al.,
      2008</a>).</p>
  <p><strong>(Towards?) Turing-complete computations.</strong> The
    content of the residual stream might represent a type of flexible
    memory with contents that can be read, modified, deleted, or merged
    at different stages in the stream. We might expect this to allow for
    pretty complex computations, probably bounded below by the
    expressive power of the transformer architecture.</p>
  <p><strong>Representation overloading.</strong> This also happens to
    mesh extremely well with a recent proposal for
    biologically-plausible backpropagation from <a
      href="https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full">Sezener
      et al. (2020)</a>:</p>
  <blockquote>
    <p>[External input to a neuron is] used for gating the weights: each
      neuron has a bank of weights at its disposal, and the external input
      determines which one is used. For example, a neuron might use one
      set of weights when the visual input contains motion cues
      predominantly to the right; another set of weights when it contains
      motion cues predominantly to the left; and yet another when there
      are no motion cues at all.</p>
  </blockquote>
  <p><strong>Model predictions.</strong> There are also
    experimentally-testable predictions<sup>[18]</sup>:</p>
  <div class="sidenote">
    <p>[18] </p>
    <p>Or perhaps post-dictions, those claims seem plausible enough that
      I might have read them before.</p>
  </div>
  <ul class="incremental">
    <li>
      <p>When task-relevant, robust representations of simple stimuli
        (f.e. edges) should be found all along layer 5 of the ventral
        stream.</p>
    </li>
    <li>
      <p>Variability of neural activity should increase not only in
        proportion to variability in the <em>immediately</em> preceding unit
        but in proportion to variability in <em>all</em> preceding
        units.</p>
    </li>
    <li>
      <p>We might expect some degree of dynamic deleting and/or
        editing in layer 5, where a portion of incoming thalamocortical
        inputs are partially or fully canceled out by computed inhibitory
        input.</p>
    </li>
  </ul>
  <p>These shouldn’t even be too difficult to test (<a
      href="https://www.cell.com/neuron/pdf/S0896-6273(19)30889-X.pdf">compared
      to, like, calcium imaging of dendrites in freely behaving
      animals</a>).</p>
  <p><strong>Model limitations.</strong> While I’d argue that the
    “residual stream” interpretation explains strictly more about
    biology than the NHBT interpretation, there are still a bunch more
    puzzle pieces<sup>[19]</sup>.</p>
  <div class="sidenote">
    <p>[19] </p>
    <p>Of course, this is neuroscience after all.</p>
  </div>
  <p>But at least there are some interesting hints for how to
    continue:</p>
  <ol class="incremental" type="1">
    <li>Which principles govern the computations within layer 2/3? I
      don’t believe that the cortical microcircuit implements the
      transformer architecture. But it also doesn’t have to - a
      “traditional” recurrent neural network should be able to do the same
      things as the transformer. It just cannot be trained as
      efficiently.</li>
    <li>How do we train the network to do “useful” computations? The
      specter of the “missing teaching signal” in the biological brain has
      prompted a lot of creative proposals from the neuroscience community
      over the decades<sup>[20]</sup>. But the recent astonishing progress
      with “<a href="https://en.wikipedia.org/wiki/Self-supervised_learning">self-supervised</a>”
      (i.e. autoregressive) learning shows that very capable networks can
      emerge without an external teaching signal. Instead of the
      backpropagation algorithm (which is not well-suited for biological
      networks), the brain might implement the next best approximation (<a
        href="https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full">Sezener
        et al. 2020</a>; <a
        href="https://proceedings.neurips.cc/paper/2018/hash/1dc3a89d0d440ba31729b0ba74b93a33-Abstract.html">Sacramento
        et al., 2018</a>)</li>
    <li>What about inhibition? <a href="https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1.full">Sezener
        et al. (2020)</a> again have an answer<sup>[21]</sup>: Inhibition
      might be responsible for gating on or off certain branches of the
      dendrites, effectively modulating the projection of the residual
      stream.</li>
  </ol>
  <p>And what about <a href="https://www.nature.com/articles/nn.4201">calcium waves in
      astrocytes</a>? And <a href="https://en.wikipedia.org/wiki/Axo-axonic_synapse">axo-axonic
      synapses</a>? And the fact that mice are <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5909038/">basically
      blind</a>? And …? Phew, great questions. I don’t know man. One step
    at a time.</p>
  <p>If you want to subscribe (it’s easy, free, and will stay free),
    click the button &lt;3</p>
  <div class="sidenote">
    <p>[2] </p>
    <p>Look, the<a href="https://en.wikipedia.org/wiki/Brodmann_area">guy who came up
        with these areas in 1909</a> didn’t actually know what they were
      good for, so let’s cut him some slack.</p>
  </div>
  <div class="sidenote">
    <p>[20] </p>
    <p>Although most proposals I’ve encountered so far have been begging
      the question, just hiding the teaching signal in ever-more-clever
      places. Self-organizing networks are the notable exception (<a
        href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138194/">Kaschube
        et al., 2010</a>).</p>
  </div>
  <div class="sidenote">
    <p>[21] </p>
    <p>The paper is not without flaws (the biological evidence is weak,
      the SARCOS dataset is contaminated to the point of non-usability,
      the results don’t quite deliver what they promise), but it has
      <em>so many</em> great ideas.
    </p>
  </div>
  <div class="debug-grid"></div>
  <script src="../../index.js"></script>
</body>

</html>